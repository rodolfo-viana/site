<!doctype html><html class="dark light" lang=en><head><meta charset=UTF-8><meta content="IE=edge" http-equiv=X-UA-Compatible><meta content="width=device-width,initial-scale=1.0" name=viewport><meta content=https://rodolfoviana.com.br name=base><title>
         Comparative Analysis of U-Net Architectures for Polyp Segmentation in Colonoscopy
        
    </title><meta content="Comparative Analysis of U-Net Architectures for Polyp Segmentation in Colonoscopy" property=og:title><meta content="Edited version of a paper written for the Deep Learning course, taught by Prof. Denis Henrique Pinheiro Salvadeo, PhD., in the master's program of the Graduate Program in Computer Science at Unesp" property=og:description><meta content="Edited version of a paper written for the Deep Learning course, taught by Prof. Denis Henrique Pinheiro Salvadeo, PhD., in the master's program of the Graduate Program in Computer Science at Unesp" name=description><meta content=https://rodolfoviana.com.br/image.png property=og:image><meta content=image/png property=og:image:type><meta content=website property=og:type><meta content=https://rodolfoviana.com.br/en/projetos/unet-comparativo/ property=og:url><link href=/favicon.png rel=icon type=image/png><link href=https://rodolfoviana.com.br/fonts.css rel=stylesheet><script src=https://rodolfoviana.com.br/js/codeblock.js></script><script src=https://rodolfoviana.com.br/js/note.js></script><script>MathJax = {
          options: {enableMenu: false},
          loader: {load: ['[tex]/boldsymbol']},
          tex: {
            packages: {'[+]': ['boldsymbol']},
            inlineMath: [['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true,
            processEnvironments: true,
            }
        }</script><link title="rodolfo viana" href=https://rodolfoviana.com.br/rss.xml rel=alternate type=application/rss+xml><link href=https://rodolfoviana.com.br/theme/dark.css rel=stylesheet><script src=https://rodolfoviana.com.br/js/themetoggle.js></script><script>setTheme("dark");</script><link href=https://rodolfoviana.com.br/main.css media=screen rel=stylesheet><script src=https://rodolfoviana.com.br/js/mobile-menu.js></script><script src=https://rodolfoviana.com.br/js/d3.min.js></script><script async id=MathJax-script src=https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js></script><script src=https://rodolfoviana.com.br/js/geolocation.js></script><body><div class=content><header><div class=main><a href=https://rodolfoviana.com.br>rodolfo viana</a></div><button aria-label=Menu class=hamburger id=mobile-menu-button><span></span><span></span><span></span></button><nav id=nav-menu><a href=https://rodolfoviana.com.br/en/projetos style=margin-left:.5em>projects</a><a href=https://rodolfoviana.com.br/en/curriculo style=margin-left:.5em>résumé</a><a href=https://rodolfoviana.com.br/en/tags style=margin-left:.5em>tags</a><a onclick="switchLanguage('pt')" title="Voltar para Português" href=javascript:void(0) style=margin-left:.5em>PT</a></nav></header><main><article><div class=title><div class=page-header>Comparative Analysis of U-Net Architectures for Polyp Segmentation in Colonoscopy<span class=primary-color style=font-size:1.6em>.</span></div><div class=meta>Published: <time>11-13-2025</time><br><span class=tags-label>Tags:</span><span class=tags> <a class=post-tag href=https://rodolfoviana.com.br/en/tags/machine-learning/>machine learning</a>, <a class=post-tag href=https://rodolfoviana.com.br/en/tags/u-net/>u-net</a>, <a class=post-tag href=https://rodolfoviana.com.br/en/tags/neural-networks/>neural networks</a>, <a class=post-tag href=https://rodolfoviana.com.br/en/tags/medicine/>medicine</a> </span></div></div><div class=toc-container><h1 class=toc-title>Table of Contents</h1><ul class=toc-list><li><a href=https://rodolfoviana.com.br/en/projetos/unet-comparativo/#introduction>Introduction</a><li><a href=https://rodolfoviana.com.br/en/projetos/unet-comparativo/#related-work>Related Work</a><li><a href=https://rodolfoviana.com.br/en/projetos/unet-comparativo/#theoretical-foundations-of-u-net-architectures>Theoretical Foundations of U-Net Architectures</a> <ul><li><a href=https://rodolfoviana.com.br/en/projetos/unet-comparativo/#u-net-nested-dense-connections>U-Net++: Nested Dense Connections</a><li><a href=https://rodolfoviana.com.br/en/projetos/unet-comparativo/#attention-u-net-spatial-attention-mechanisms>Attention U-Net: Spatial Attention Mechanisms</a><li><a href=https://rodolfoviana.com.br/en/projetos/unet-comparativo/#resu-net-residual-connections-for-deeper-networks>ResU-Net: Residual Connections for Deeper Networks</a></ul><li><a href=https://rodolfoviana.com.br/en/projetos/unet-comparativo/#methodology>Methodology</a> <ul><li><a href=https://rodolfoviana.com.br/en/projetos/unet-comparativo/#dataset-and-preprocessing>Dataset and Preprocessing</a><li><a href=https://rodolfoviana.com.br/en/projetos/unet-comparativo/#implementation-and-training>Implementation and Training</a><li><a href=https://rodolfoviana.com.br/en/projetos/unet-comparativo/#evaluation-metrics-and-statistical-analysis>Evaluation Metrics and Statistical Analysis</a></ul><li><a href=https://rodolfoviana.com.br/en/projetos/unet-comparativo/#results>Results</a> <ul><li><a href=https://rodolfoviana.com.br/en/projetos/unet-comparativo/#comparative-performance-analysis-by-context>Comparative Performance Analysis by Context</a><li><a href=https://rodolfoviana.com.br/en/projetos/unet-comparativo/#qualitative-analysis-of-segmentation-results>Qualitative Analysis of Segmentation Results</a></ul><li><a href=https://rodolfoviana.com.br/en/projetos/unet-comparativo/#conclusion>Conclusion</a><li><a href=https://rodolfoviana.com.br/en/projetos/unet-comparativo/#future-work>Future Work</a><li><a href=https://rodolfoviana.com.br/en/projetos/unet-comparativo/#note-on-ai-use>Note on AI Use</a><li><a href=https://rodolfoviana.com.br/en/projetos/unet-comparativo/#references>References</a></ul></div><section class=body><h1 id=introduction>Introduction</h1><p>Colorectal cancer is one of the most prevalent tumors worldwide, accounting for approximately 10% of cases and over 930,000 annual deaths<sup class=footnote-reference id=fr-1-1><a href=#fn-1>1</a></sup>. The progression from adenomatous polyps to carcinoma typically takes between 10 and 15 years, offering a wide window for prevention through early detection and removal<sup class=footnote-reference id=fr-2-1><a href=#fn-2>2</a></sup>; however, the detection rate during colonoscopy varies widely, especially due to differences in polyp morphology<sup class=footnote-reference id=fr-3-1><a href=#fn-3>3</a></sup>.<p>In this context, data science, particularly machine learning, has much to offer. Polyp segmentation can be treated as a pixel-by-pixel classification problem. Such work is challenging due to the morphological heterogeneity of lesions, low contrast between polyp tissue and adjacent mucosa, and variable lighting conditions in endoscopy; however, advances in convolutional neural networks have driven significant improvements in segmentation accuracy, surpassing traditional methods based on manual features<sup class=footnote-reference id=fr-4-1><a href=#fn-4>4</a></sup> <sup class=footnote-reference id=fr-5-1><a href=#fn-5>5</a></sup>.<p>In this scenario, U-Net has established itself as the state-of-the-art for such tasks through an encoder-decoder scheme with skip connections<sup class=footnote-reference id=fr-6-1><a href=#fn-6>6</a></sup>.</p><img style="border:5px solid #ef5350;width:75%;max-width:75%;height:auto;margin:0 auto;display:block" src=./unet.png><p>However, its limitations have motivated variants aimed at improving representation fusion and feature selectivity. This work performs systematic comparisons under controlled experimental conditions, comparing five architectures in polyp segmentation using the Kvasir-SEG dataset<sup class=footnote-reference id=fr-7-1><a href=#fn-7>7</a></sup>.<h1 id=related-work>Related Work</h1><p>Automated polyp segmentation has evolved significantly from classical methods to specialized deep architectures. Initial approaches relied on manual features and traditional computer vision techniques. Bernal et al.<sup class=footnote-reference id=fr-3-2><a href=#fn-3>3</a></sup> proposed WM-DOVA maps combining intensity and shape information, while Tajbakhsh et al.<sup class=footnote-reference id=fr-5-2><a href=#fn-5>5</a></sup> employed shallow convolutional networks with contextual features for detection in colonoscopy videos.<p>The introduction of fully convolutional networks (FCN)<sup class=footnote-reference id=fr-8-1><a href=#fn-8>8</a></sup> revolutionized semantic segmentation by eliminating final dense layers and enabling pixel-by-pixel predictions. Brandão et al.<sup class=footnote-reference id=fr-4-2><a href=#fn-4>4</a></sup> adapted FCNs for polyp segmentation, demonstrating superiority over traditional methods but still facing difficulties with imprecise edges and morphological variations. The U-Net architecture<sup class=footnote-reference id=fr-6-2><a href=#fn-6>6</a></sup>, with its encoder-decoder paradigm and skip connections, became the reference for biomedical segmentation by preserving spatial information at multiple scales.<p>Various U-Net extensions have been proposed for polyp segmentation. Zhou et al.<sup class=footnote-reference id=fr-9-1><a href=#fn-9>9</a></sup> introduced nested dense connections to reduce semantic gaps, while Oktay et al.<sup class=footnote-reference id=fr-10-1><a href=#fn-10>10</a></sup> incorporated spatial attention mechanisms. Jha et al.<sup class=footnote-reference id=fr-11-1><a href=#fn-11>11</a></sup> proposed DoubleU-Net, combining two encoder-decoder paths with ASPP (Atrous Spatial Pyramid Pooling) blocks, achieving a Dice score of 82.3% on Kvasir-SEG.<p>Recent approaches explore sophisticated attention mechanisms and multi-scale aggregation. Fan et al.<sup class=footnote-reference id=fr-12-1><a href=#fn-12>12</a></sup> developed PraNet with parallel reverse attention, reporting 89.8% Dice on Kvasir-SEG. Fang et al.<sup class=footnote-reference id=fr-13-1><a href=#fn-13>13</a></sup> proposed selective feature aggregation with area-boundary constraints, while Zhang et al.<sup class=footnote-reference id=fr-14-1><a href=#fn-14>14</a></sup> introduced adaptive context selection to handle morphological variability. Srivastava et al.<sup class=footnote-reference id=fr-15-1><a href=#fn-15>15</a></sup> proposed MSRF-Net with multi-scale residual fusion.<p>More recent work investigates transformers for medical segmentation. Valanarasu et al.<sup class=footnote-reference id=fr-16-1><a href=#fn-16>16</a></sup> introduced Medical Transformer with axial attention, demonstrating competitiveness with CNNs. Huang et al.<sup class=footnote-reference id=fr-17-1><a href=#fn-17>17</a></sup> proposed HarDNet-MSEG, achieving Dice scores above 90% at 86 FPS, balancing accuracy and computational efficiency.<p>Despite advances, direct comparisons are hindered by methodological variations: different data partitions, augmentation strategies, loss functions, and hyperparameters. Many works report results on multiple datasets but lack rigorous statistical analysis. This study addresses these gaps through systematic comparison of five fundamental U-Net architectures under controlled experimental conditions, with multiple seeds for variance analysis, providing a reproducible benchmark for the domain.<h1 id=theoretical-foundations-of-u-net-architectures>Theoretical Foundations of U-Net Architectures</h1><p>U-Net establishes the encoder-decoder paradigm with skip connections for biomedical segmentation<sup class=footnote-reference id=fr-6-3><a href=#fn-6>6</a></sup>. The encoder captures multi-scale semantic information through successive convolutions and pooling, while skip connections concatenate features from encoder to decoder, preserving spatial details.<p>Let \(x\in\mathbb{R}^{H\times W\times C}\) be an input image and \(y\in{{0,1}}^{H\times W}\) its binary reference mask. The network implements a mapping \(f:\mathbb{R}^{H\times W\times C}\rightarrow[0,1]^{H\times W}\) whose threshold at 0.5 produces \(\hat{y}\in{{0,1}}^{H\times W}\). At each encoder level \(l\), the number of channels (width) follows \(c_l = c_0\cdot 2^{l}\).<h2 id=u-net-nested-dense-connections>U-Net++: Nested Dense Connections</h2><p>U-Net++ addresses the semantic gap of U-Net through nested convolutional blocks<sup class=footnote-reference id=fr-9-2><a href=#fn-9>9</a></sup>. In U-Net, skip connections concatenate semantically distant features (low level in encoder, high level in decoder). U-Net++ resolves this by gradually connecting features through dense paths. Each node \(\mathbf{X}^{i,j}\) is<p>\[ \begin{aligned} \mathbf{X}^{i,j} = \begin{cases} \mathcal{H}(\mathbf{X}^{i,j-1}), & j = 0 \\ \mathcal{H}\left(\left[\left[\mathbf{X}^{i,k}\right]_{k=0}^{j-1}, \mathcal{U}(\mathbf{X}^{i+1,j-1})\right]\right), & j > 0, \end{cases} \end{aligned} \]<p>where \(i\) indexes resolution, \(j\) connection density, \(\mathcal{H}(\cdot)\) convolution, \(\mathcal{U}(\cdot)\) upsampling, \([[\cdot]]\) concatenation. This progressively reduces the semantic gap through successive transformations.<p>In addition to U-Net++, we also adopt a version with deep supervision, which adds multi-scale supervision \(\mathcal{L}_{\text{total}} = \sum_{j=1}^{J} \omega_j \mathcal{L}(\mathbf{y}, \mathbf{\hat{y}}^{(j)})\), mitigating vanishing gradients and accelerating convergence.<h2 id=attention-u-net-spatial-attention-mechanisms>Attention U-Net: Spatial Attention Mechanisms</h2><p>Attention U-Net incorporates attention gates that spatially weight features before concatenation<sup class=footnote-reference id=fr-10-2><a href=#fn-10>10</a></sup>, suppressing irrelevant information transmitted by skip connections. Given \(\mathbf{x}\) from the encoder and gating signal \(\mathbf{g}\) from the decoder, attention coefficients are<p>\[ \hat{\mathbf{x}} = \sigma_2\left(\psi^T \sigma_1(\mathbf{W}_x^T \mathbf{x} + \mathbf{W}_g^T \mathbf{g})\right) \odot \mathbf{x} \]<p>where \(\sigma_1\) and \(\sigma_2\) are ReLU and sigmoid respectively, allowing suppression of irrelevant regions and emphasis of salient structures.<h2 id=resu-net-residual-connections-for-deeper-networks>ResU-Net: Residual Connections for Deeper Networks</h2><p>ResU-Net integrates the U-Net architecture with residual blocks inspired by ResNet<sup class=footnote-reference id=fr-18-1><a href=#fn-18>18</a></sup>, addressing the performance degradation that occurs when training very deep networks<sup class=footnote-reference id=fr-19-1><a href=#fn-19>19</a></sup>. While standard U-Net uses double convolution blocks, ResU-Net replaces these blocks with residual blocks that implement identity connections. A residual block learns a residual function \(\mathcal{F}(\mathbf{x})\) instead of the desired direct mapping \(\mathcal{H}(\mathbf{x})\), through the shortcut connection<p>\[ \mathbf{y} = \mathcal{F}(\mathbf{x}, {W_i}) + \mathbf{x}, \]<p>where \(\mathbf{x}\) and \(\mathbf{y}\) are input and output vectors, and \(\mathcal{F}(\mathbf{x}, {W_i})\) represents the residual transformation to be learned. This architecture facilitates training deeper networks by allowing the model to learn incremental transformations instead of complete mappings, resulting in better gradient propagation and potential for capturing richer representations.<h1 id=methodology>Methodology</h1><h2 id=dataset-and-preprocessing>Dataset and Preprocessing</h2><p>Kvasir-SEG<sup class=footnote-reference id=fr-7-2><a href=#fn-7>7</a></sup> contains 1,000 colonoscopy images with pixel-by-pixel segmentation masks and bounding boxes, covering wide diversity in polyp size and shape. Images have varying resolutions (from 332\(\times\)487 to 1920\(\times\)1072), and are resized to 256\(\times\)256 pixels and normalized with ImageNet statistics. The partition considers 80% of images for training, 10% for validation, 10% for testing.<h2 id=implementation-and-training>Implementation and Training</h2><p>Our study examines five U-Net variants: standard U-Net (double 3\(\times\)3 convolution blocks, batch normalization, ReLU, five levels with \({64, 128, 256, 512, 512}\) channels); U-Net++ (nested dense paths, with and without deep supervision); Attention U-Net (with attention gates); ResU-Net (residual blocks with identity connections, five levels with \({64, 128, 256, 512, 512}\) channels).<p>All models were trained for a maximum of 100 epochs, with batch size 8, using AdamW optimizer (initial learning rate \(10^{-4}\), weight decay \(10^{-5}\)). Learning rate scheduling used 5 warmup epochs followed by reduction on plateau (factor 0.5, patience 5, minimum rate \(10^{-6}\)). The combined loss function is \(\mathcal{L} = 0{,}4 \mathcal{L}_{\text{BCE}} + 0{,}6 \mathcal{L}_{\text{Dice}}\). Augmentation includes flips, rotation, affine transformations, color jitter, Gaussian blur. Training adopts early stopping monitoring validation Dice score with patience of 10 epochs, stopping when no improvement occurs. To ensure reproducibility, each architecture was trained with three random seeds (42, 123, 456).<h2 id=evaluation-metrics-and-statistical-analysis>Evaluation Metrics and Statistical Analysis</h2><p>Our evaluation framework employs five complementary metrics that capture different aspects of segmentation performance, providing a comprehensive appreciation of model capabilities in various clinical scenarios, using information from false negatives (FN), false positives (FP), true positives (TP), and true negatives (TN).<p>The Dice Similarity Coefficient (Dice score) measures regional overlap between predicted \(P\) and ground truth \(V\) segmentations — for pixel-wise binary segmentation, the Dice score is equivalent to the F1 score. It is obtained with<p>\[ \frac{2 \mid P \cap V \mid}{\mid P \mid + \mid V \mid}. \]<p>The Jaccard Index (Intersection over Union, or IoU) provides an alternative overlap measure from<p>\[ \frac{\mid P \cap V \mid}{\mid P \cup V \mid}. \]<p>In addition to these main metrics, we calculate sensitivity, which measures the model's ability to detect polyp pixels; specificity, which quantifies the proportion of non-polyp pixels correctly identified; and precision, which evaluates the proportion of positive predictions that are correct.<h1 id=results>Results</h1><p>Standard U-Net achieved the best overall performance (Dice: 83.66\(\pm\)0.52%, IoU: 75.65\(\pm\)0.29%), demonstrating remarkable consistency with the lowest standard deviation among the three seeds. U-Net++ with deep supervision obtained comparable performance (Dice: 83.48\(\pm\)0.89%), presenting the highest sensitivity (89.39\(\pm\)0.88%), indicating superior capability for detecting polyp pixels.<table><thead><tr><th>Architecture<th style=text-align:center>Dice<br>(%)<th style=text-align:center>IoU<br>(%)<th style=text-align:center>Sensit.<br>(%)<th style=text-align:center>Specif.<br>(%)<th style=text-align:center>Precision<br>(%)<tbody><tr><td>U-Net<td style=text-align:center>83.66\(\pm\)0.52<td style=text-align:center>75.65\(\pm\)0.29<td style=text-align:center>88.36\(\pm\)0.83<td style=text-align:center>97.28\(\pm\)0.22<td style=text-align:center>84.76\(\pm\)0.27<tr><td>U-Net++<td style=text-align:center>82.24\(\pm\)1.33<td style=text-align:center>73.64\(\pm\)1.46<td style=text-align:center>87.52\(\pm\)2.51<td style=text-align:center>97.22\(\pm\)0.57<td style=text-align:center>83.24\(\pm\)1.03<tr><td>U-Net++ (DS)<td style=text-align:center>83.48\(\pm\)0.89<td style=text-align:center>75.46\(\pm\)1.03<td style=text-align:center>89.39\(\pm\)0.88<td style=text-align:center>97.13\(\pm\)0.30<td style=text-align:center>83.42\(\pm\)1.09<tr><td>Attention U-Net<td style=text-align:center>82.24\(\pm\)0.23<td style=text-align:center>73.63\(\pm\)0.16<td style=text-align:center>88.74\(\pm\)0.67<td style=text-align:center>96.82\(\pm\)0.06<td style=text-align:center>82.01\(\pm\)0.12<tr><td>ResU-Net<td style=text-align:center>82.43\(\pm\)0.52<td style=text-align:center>73.85\(\pm\)0.88<td style=text-align:center>86.60\(\pm\)0.63<td style=text-align:center>97.52\(\pm\)0.28<td style=text-align:center>84.35\(\pm\)1.06</table><p>ResU-Net, incorporating residual connections, obtained Dice of 82.43\(\pm\)0.52% and stood out with the highest specificity (97.52\(\pm\)0.28%), indicating excellent capability to reject false detections. Standard U-Net++ and Attention U-Net presented similar performance (Dice: 82.24%), with Attention U-Net exhibiting slightly lower specificity (96.82%), suggesting that attention mechanisms may occasionally compromise non-polyp tissue discrimination.<p>All architectures maintained specificity above 96.8%, essential for clinical applications where false positives reduce endoscopist confidence. Standard U-Net also achieved the best precision (84.76\(\pm\)0.27%), confirming its robustness. Results reveal that simpler architectures can offer competitive performance with greater consistency, questioning the cost-benefit of complex architectural modifications for this specific domain.<h2 id=comparative-performance-analysis-by-context>Comparative Performance Analysis by Context</h2><p>Analysis of architecture performance in the three distinct contexts (training, validation, and test) presented in the figure below reveals fundamental patterns about model generalization behavior. All architectures present a clear performance hierarchy, confirming the progressive task difficulty and adequate methodology.</p><img style="border:5px solid #ef5350;width:75%;max-width:75%;height:auto;margin:0 auto;display:block" src=./graph.png><p>Standard U-Net demonstrates excellent consistency between validation and test, with minimal gap (\(\sim\)1.5 pp.), indicating robust generalization capability. Its low variability between seeds, visible in compact boxplots, suggests stable and predictable training — a desirable characteristic for clinical applications where reproducibility is critical.<p>U-Net++ with deep supervision presents similar behavior, maintaining good consistency and reduced gap between validation and test. The multi-scale supervision strategy effectively contributes to training stabilization, resulting in elevated sensitivity without significantly sacrificing other metrics. ResU-Net exhibits intermediate variability, with residual connections providing stable training but without decisive advantages in final performance.<p>Attention U-Net and standard U-Net++ show slightly larger gaps between validation and test, with Attention U-Net occasionally presenting lower specificity. This behavior questions the effectiveness of spatial attention mechanisms for this domain, suggesting that additional selectivity may not be necessary or may even be counterproductive when the object of interest presents great morphological variability.<h2 id=qualitative-analysis-of-segmentation-results>Qualitative Analysis of Segmentation Results</h2><p>Visual inspection of predictions, as shown in the figure below, reveals significant and clinically relevant differences between architectures, particularly in challenging cases. The analysis considers five representative cases that expose distinct performance characteristics of the architectures.<figure style="width:85%;max-width:85%;height:auto;margin:0 auto;display:block"><img style="border:5px solid #ef5350" src=./mask_comparison.png><figcaption style=color:#555;margin-top:1px;font-size:.8em;line-height:1.5>From left to right: original image, ground truth, predictions from U-Net, U-Net++, U-Net++ with deep supervision, Attention U-Net, ResU-Net</figcaption></figure><p>The first row shows a pedunculated polyp with right lateral lobe; only U-Net++ and U-Net++ with deep supervision recover this extension, reproducing the complete morphology. U-Net, Attention U-Net, and ResU-Net restrict themselves to the main body; among those covering integrally, the variant with deep supervision is the most consistent, with a discrete peripheral halo.<p>In the second row, with shiny mucosa, low contrast, and reflections, all architectures exhibit some degree of leakage or halo. U-Net and U-Net++ (standard and with deep supervision) expand the predicted area, but with diffuse contours; Attention U-Net and ResU-Net mitigate leakages but still lose definition in poorly lit edges. None fully reproduces the annotated contour.<p>The third row presents a small target near the wall and transition to dark lumen. U-Net undersegments; U-Net++ better approximates the shape; U-Net++ with deep supervision stabilizes the mask. Attention U-Net and ResU-Net provide the cleanest predictions, with fewer false positives in the background.<p>The fourth row represents a favorable case: clean visual field, relatively homogeneous background, and well-defined polyp boundaries. All achieve high accuracy, with only incremental gains. U-Net++ (with and without deep supervision) refines regularity; Attention U-Net avoids leakages; ResU-Net practically coincides with the ground truth.<p>Finally, the fifth row includes occlusion by instrument and deep shadows. U-Net undersegments at the base; U-Net++ improves continuity but creates an over-segmentation "collar"; U-Net++ with deep supervision recovers volume, rounding the occluded region. Attention U-Net and ResU-Net prove more robust, preserving lesion mass and continuity.<p>Architectural differences emerge especially in challenging cases. U-Net and U-Net++ are more susceptible to leakages and edge imprecision; deep supervision attenuates such effects modestly and not universally. In the examples, the incidence of false positives is low in complex backgrounds, which is clinically promising; nevertheless, such impressions must be corroborated by quantitative metrics on the complete dataset.<h1 id=conclusion>Conclusion</h1><p>This study compared five U-Net architecture variants for polyp segmentation in colonoscopy. Standard U-Net achieved the best overall performance (Dice: 83.66\(\pm\)0.52%), demonstrating that architectural simplicity and consistency can be advantageous. Its low standard deviation indicates stable and reproducible training, an essential characteristic for clinical applications where predictability is crucial.<p>U-Net++ with deep supervision obtained comparable performance (Dice: 83.48\(\pm\)0.89%) and higher sensitivity (89.39%), suggesting that multi-scale supervision can improve polyp detection without compromising other metrics. ResU-Net stood out in specificity (97.52%), indicating that residual connections contribute to better non-polyp tissue discrimination, though without decisive advantages in Dice score.<p>Standard U-Net++ and Attention U-Net presented similar but inferior performance (Dice: 82.24%). The absence of significant gains for Attention U-Net questions the effectiveness of spatial attention mechanisms in this domain, especially considering the additional complexity. Results suggest that polyp characteristics — high morphological variability and low contrast — may not benefit from additional spatial selectivity.<p>All architectures maintained specificity above 96.8%, essential for clinical acceptance where false positives reduce endoscopist confidence. Primary differences manifest in sensitivity and edge delineation, critical metrics for precise endoscopic interventions. Results highlight the value of well-calibrated fundamental architectures and question the cost-benefit of complex architectural modifications for polyp segmentation.<h1 id=future-work>Future Work</h1><p>This study establishes promising directions for future research in automated polyp segmentation. First, cross-validation on additional datasets (CVC-ClinicDB, ETIS-Larib, CVC-ColonDB) would strengthen the generalization of findings and reveal possible biases specific to Kvasir-SEG. Evaluation on data from multiple hospital centers, with different endoscopic equipment and acquisition protocols, is essential to verify clinical robustness.<p>Future work should incorporate computational cost analysis, including number of parameters, inference time, and memory consumption. Such metrics are critical for deployment in real-time diagnostic aid systems during colonoscopy. Comparison with recent transformer-based architectures and hybrid CNN-transformer methods also merits investigation, considering the balance between accuracy and computational efficiency demonstrated by HarDNet-MSEG<sup class=footnote-reference id=fr-17-2><a href=#fn-17>17</a></sup>.<p>Statistical analysis can be expanded through paired significance tests (Student's t-test, Wilcoxon test) to determine if observed differences between architectures are statistically significant. Confidence intervals would complement variance analysis between seeds, providing more robust estimates of model uncertainty.<p>Alternative training strategies merit systematic exploration. Investigation of different loss functions (Focal Loss, Tversky Loss, Boundary Loss) may improve edge delineation, a persistent challenge observed in qualitative analysis. Ensemble techniques combining predictions from multiple architectures may increase robustness, particularly in challenging cases with low contrast or occlusions.<p>Finally, prospective clinical studies with endoscopists evaluating automatic systems in real scenarios are fundamental for translational validation. User-centered metrics, including procedure time, adenoma detection rate, and clinical acceptability, should complement purely technical metrics to guide development of clinically useful and safe systems.<h1 id=note-on-ai-use>Note on AI Use</h1><p>The authors declare that no generative artificial intelligence tool was used for study conception, code creation, results analysis, illustration generation, or article writing, except for spelling and grammatical review, for which Claude Opus 4.1 was used.<h1 id=references>References</h1><section class=footnotes><ol class=footnotes-list><li id=fn-1><p>World Health Organization, "Colorectal cancer", https://www.who.int/news-room/fact-sheets/detail/colorectal-cancer, Jul. 2023, online. Accessed September 25, 2025. <a href=#fr-1-1>↩</a></p><li id=fn-2><p>D. A. Corley, C. D. Jensen, A. R. Marks, W. K. Zhao, J. K. Lee, C. A. Doubeni, A. G. Zauber, J. de Boer, B. H. Fireman, J. E. Schottinger, and T. R. Levin, "Adenoma detection rate and risk of colorectal cancer and death", <em>The New England Journal of Medicine</em>, vol. 370, no. 14, pp. 1298–1306, 2014. <a href=#fr-2-1>↩</a></p><li id=fn-3><p>J. Bernal, F. J. Sánchez, G. Fernández-Esparrach, D. Gil, C. Rodríguez, and F. Vilariño, "WM-DOVA maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians", <em>Computerized Medical Imaging and Graphics</em>, vol. 43, pp. 99–111, 2015. <a href=#fr-3-1>↩</a> <a href=#fr-3-2>↩2</a></p><li id=fn-4><p>P. Brandão, E. Mazomenos, G. Ciuti, R. Caliò, F. Bianchi, A. Menciassi, P. Dario, A. Koulaouzidis, A. Arezzo, and D. Stoyanov, "Fully convolutional neural networks for polyp segmentation in colonoscopy", in <em>Medical Imaging 2017: Computer-Aided Diagnosis, ser. Proceedings of SPIE</em>, vol. 10134, Orlando, FL, USA, 2017. <a href=#fr-4-1>↩</a> <a href=#fr-4-2>↩2</a></p><li id=fn-5><p>N. Tajbakhsh, S. R. Gurudu, and J. Liang, "Automated polyp detection in colonoscopy videos using shape and context information", <em>IEEE Transactions on Medical Imaging</em>, vol. 35, no. 2, pp. 630–644, 2016. <a href=#fr-5-1>↩</a> <a href=#fr-5-2>↩2</a></p><li id=fn-6><p>O. Ronneberger, P. Fischer, and T. Brox, "U-net: Convolutional networks for biomedical image segmentation", in <em>Medical Image Computing and Computer-Assisted Intervention - MICCAI 2015, ser. Lecture Notes in Computer Science</em>, vol. 9351. Cham: Springer, 2015, pp. 234–241. <a href=#fr-6-1>↩</a> <a href=#fr-6-2>↩2</a> <a href=#fr-6-3>↩3</a></p><li id=fn-7><p>D. Jha, P. H. Smedsrud, M. A. Riegler, P. Halvorsen, T. de Lange, D. Johansen, and H. D. Johansen, "Kvasir-SEG: A segmented polyp dataset", in <em>MultiMedia Modeling (MMM 2020), Proceedings, Part II, ser. Lecture Notes in Computer Science</em>, vol. 11962. Cham: Springer, 2020, pp. 451–462. <a href=#fr-7-1>↩</a> <a href=#fr-7-2>↩2</a></p><li id=fn-8><p>E. Shelhamer, J. Long, and T. Darrell, "Fully convolutional networks for semantic segmentation", <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, vol. 39, no. 4, pp. 640–651. 2017. <a href=#fr-8-1>↩</a></p><li id=fn-9><p>Z. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, "Unet++: A nested u-net architecture for medical image segmentation", in <em>4th International Workshop, DLMIA 2018, and 8th International Workshop, ML-CDS 2018, Held in Conjunction with MICCAI 2018</em>, vol. 11045, 2018, pp. 3–11. <a href=#fr-9-1>↩</a> <a href=#fr-9-2>↩2</a></p><li id=fn-10><p>O. Oktay, J. Schlemper, L. L. Folgoc, M. Lee, M. Heinrich, K. Misawa, K. Mori, S. McDonagh, N. Y. Hammerla, B. Kainz et al., "Attention u-net: Learning where to look for the pancreas", arXiv preprint arXiv:1804.03999, 2018. <a href=#fr-10-1>↩</a> <a href=#fr-10-2>↩2</a></p><li id=fn-11><p>D. Jha, P. H. Smedsrud, M. A. Riegler, H. D. Johansen, T. de Lange, P. Halvorsen, and D. Johansen, "DoubleU-Net: A deep convolutional neural network for medical image segmentation", in <em>2020 IEEE 33rd International Symposium on Computer-Based Medical Systems (CBMS)</em>. IEEE, 2020, pp. 558–564. <a href=#fr-11-1>↩</a></p><li id=fn-12><p>D.-P. Fan, G.-P. Ji, T. Zhou, G. Chen, H. Fu, J. Shen, and L. Shao, "PraNet: Parallel reverse attention network for polyp segmentation", in <em>Medical Image Computing and Computer-Assisted Intervention – MICCAI 2020</em>. Springer International Publishing, 2020, pp. 263–273. <a href=#fr-12-1>↩</a></p><li id=fn-13><p>Y. Fang, C. Chen, Y. Yuan, and K.-y. Tong, "Selective feature aggregation network with area-boundary constraints for polyp segmentation", in <em>Medical Image Computing and Computer-Assisted Intervention – MICCAI 2019</em>. Springer International Publishing, 2019, pp. 302–310. <a href=#fr-13-1>↩</a></p><li id=fn-14><p>R. Zhang, G. Li, Z. Li, S. Cui, D. Qian, and Y. Yu, "Adaptive context selection for polyp segmentation", in <em>Medical Image Computing and Computer-Assisted Intervention – MICCAI 2020</em>. Springer International Publishing, 2020, pp. 253–262. <a href=#fr-14-1>↩</a></p><li id=fn-15><p>A. Srivastava, D. Jha, S. Chanda, U. Pal, H. D. Johansen, D. Johansen, M. A. Riegler, S. Ali, and P. Halvorsen, "MSRF-Net: A multi-scale residual fusion network for biomedical image segmentation", <em>IEEE Journal of Biomedical and Health Informatics</em>, vol. 26, no. 5, pp. 2252–2263, 2022. <a href=#fr-15-1>↩</a></p><li id=fn-16><p>J. M. J. Valanarasu, P. Oza, I. Hacihaliloglu, and V. M. Patel, "Medical transformer: Gated axial-attention for medical image segmentation", in <em>Medical Image Computing and Computer-Assisted Intervention – MICCAI 2021</em>. Springer International Publishing, 2021, pp. 36–46. <a href=#fr-16-1>↩</a></p><li id=fn-17><p>C.-H. Huang, H.-Y. Wu, and Y.-L. Lin, "HarDNet-MSEG: A simple encoder-decoder polyp segmentation neural network that achieves over 0.9 mean dice and 86 fps", arXiv preprint arXiv:2101.07172, 2021. <a href=#fr-17-1>↩</a> <a href=#fr-17-2>↩2</a></p><li id=fn-18><p>K. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning for image recognition", in <em>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>. IEEE, 2016, pp. 770–778. <a href=#fr-18-1>↩</a></p><li id=fn-19><p>Z. Zhang, Q. Liu, and Y. Wang, "Road extraction by deep residual u-net", <em>IEEE Geoscience and Remote Sensing Letters</em>, vol. 15, no. 5, pp. 749–753, 2018. <a href=#fr-19-1>↩</a></p></ol></section></section></article></main><footer class=footer><div class=footer-content><div class=footer-left>© 2026 Rodolfo Viana</div><div class=footer-right>Made with <a href=https://www.getzola.org/ rel=noopener target=_blank>Zola</a>, <a href=https://github.com/not-matthias/apollo rel=noopener target=_blank>Apollo</a></div></div></footer></div>