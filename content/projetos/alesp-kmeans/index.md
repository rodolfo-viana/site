---
title: "Detec√ß√£o de anomalias em gastos dos deputados estaduais com K-Means"
date: 2023-09-01
toc: true
---

{{< warning >}}
O texto apresentado √© uma vers√£o editada do meu trabalho de conclus√£o do MBA em Data Science e Analytics na USP-Esalq, sob orienta√ß√£o da Prof¬™ Dr¬™ Ana Julia Righetto.
{{< /warning >}}

## Introdu√ß√£o

Cada um dos 94 parlamentares da Assembleia Legislativa do Estado de S√£o Paulo [Alesp] tem direito aos Aux√≠lio-Encargos Gerais de Gabinete de Deputado e Aux√≠lio-Hospedagem, referenciados conjuntamente como "verba de gabinete". Tal direito foi conferido pela resolu√ß√£o 783, artigo 11, de 1¬∫ de julho de 1997[^1], e se trata de um valor mensal devido pelo Estado aos deputados a fim de que eles possam cobrir despesas inerentes ao pleno exerc√≠cio das atividades parlamentares. A resolu√ß√£o estipula o limite m√°ximo da verba de gabinete em 1.250 unidades fiscais do Estado de S√£o Paulo [Ufesp]. Em 2022, com o valor da Ufesp em R$ 31,97[^2], o limite mensal que poderia ser ressarcido por deputado foi de R$ 39.962,50.

Naquele ano, o valor total empenhado para custeio da verba de gabinete perfez R$ 26.652.243,51[^3]. O montante foi 24,43% maior que a soma em 2021, de R$ 21.419.316,88[^3], e menor do que o valor anotado na rubrica para 2023, de R$ 28.607.099,96[^3]. Caso este montante se cumpra neste ano, ser√° a primeira vez que o valor ultrapassa R$ 28,5 milh√µes desde 2018.

Tais somas de recursos p√∫blicos s√£o escrutinadas por √≥rg√£os de controle que, n√£o raro, questionam sua finalidade. Um exemplo √© o processo investigat√≥rio SEI 29.0001.0246360.2021-54[^4], que apura o pagamento por loca√ß√£o de im√≥veis pertencentes a aliados pol√≠ticos do deputado Murilo Felix e nunca utilizados. Outro exemplo √© a a√ß√£o penal 0037174-14.2021.8.26.0000[^5], que aponta, entre outros elementos, o ressarcimento de despesas nunca efetuadas por parte do deputado Rog√©rio Nogueira.

Com este contexto, este projeto busca ser um instrumento para avalia√ß√£o de malversa√ß√£o de dinheiro p√∫blico por meio de _unsupervised machine learning_. Seu objetivo n√£o √© afirmar peremptoriamente se determinada despesa √© fraudulenta ou n√£o; seu escopo √© servir de ferramenta para uma observa√ß√£o inicial dos gastos, que podem ser analisados por meio de clusteriza√ß√£o, onde se objetiva encontrar um grupo de despesas cujos valores s√£o an√¥malos.

## M√©todo

### An√°lise explorat√≥ria

A primeira etapa consistiu na captura dos dados a partir do Portal de Dados Abertos da Alesp[^6], onde est√£o dispon√≠veis arquivos no formato `xml` que datam desde 2002 e cont√™m elementos que indicam o per√≠odo de refer√™ncia ("Ano", "M√™s"), al√©m de informa√ß√µes tanto do parlamentar ("Matr√≠cula", "Deputado") quanto da despesa ("Fornecedor", "CNPJ", "Tipo", "Valor"). Para este trabalho, foram ignorados os nomes dos parlamentares a fim de desconsiderar eventuais vieses ideol√≥gicos. Dado o contexto temporal dos gastos, "Ano" e "M√™s" foram usados t√£o somente para realizar a defla√ß√£o dos valores at√© 31 de dezembro de 2022 seguindo o √≠ndice de pre√ßo ao consumidor amplo [IPCA][^7]. A temporalidade das despesas, portanto, foi descartada.

Foram inseridas neste estudo apenas as despesas relacionadas a alimenta√ß√£o e hospedagem compreendidas entre os anos de 2018 e 2022. Descartaram-se, ainda, fornecedores com menos de 20 despesas no quinqu√™nio, haja vista a necessidade de se ter n√∫mero significativo para a realiza√ß√£o de clusteriza√ß√£o.

### Algoritmo de K-Means

Implementou-se um algoritmo de clusteriza√ß√£o por K-Means com a finalidade de processar esses registros. Em linhas gerais, K-Means √© um algoritmo que particiona um conjunto de pontos de dados em clusters n√£o sobrepostos, sendo pr√©-determinada a quantidade de clusters[^8]. Cada ponto de dado pertence ao cluster com a menor dist√¢ncia m√©dia entre ele e um centro (centroide).

Dado um conjunto de observa√ß√µes \\(x = \lbrace x_1, x_2, ..., x_n\rbrace \\), o algoritmo reparte as \\(n\\) observa√ß√µes em \\(k \(\geq n\)\\) conjuntos \\(S = \lbrace S_1, S_2, ..., S_k \rbrace\\) a fim de minimizar a soma dos quadrados dentro do cluster.

$$
\sum_{i = 1}^{k}\sum_{x \in S_i}{\Vert x - \mu_i \Vert}^2
$$

onde,

- \\(k\\): n√∫mero de clusters
- \\(ùëÜ_ùëñ\\): cluster \\(i\\)
- \\(ùë•\\): ponto de dado
- \\(\mu_i\\): m√©dia da dist√¢ncia dos pontos em \\(S_i\\)

Considerando que o conjunto de dados deste trabalho √© univariado e o algoritmo aplicado visa encontrar anomalias, visualmente temos:

1. Os pontos s√£o distribu√≠dos conforme seus valores:<br><img src="111.svg" width="100%">
2. Com a quantidade de clusters pr√©-determinada, s√£o calculados os centroides a partir da minimiza√ß√£o do quadrado das dist√¢ncias:<br><img src="222.svg" width="100%">
3. Os pontos pr√≥ximos aos centroides formam clusters:<br><img src="333.svg" width="100%">
4. Os pontos que n√£o se encontram nos clusters s√£o considerados anomalias:<br><img src="444.svg" width="100%">

A aplica√ß√£o de K-Means, por√©m, imp√µe algumas necessidades a este trabalho, tais como determina√ß√£o pr√©via da quantidade de clusters, um m√©todo de inicializa√ß√£o de centroides que considere m√≠nimo global em vez de m√≠nimo local, crit√©rio para converg√™ncia ideal dos centroides e valida√ß√£o dos resultados. Para aplacar tais limita√ß√µes, foram utilizados, respectivamente, o m√©todo do cotovelo, o m√©todo K-Means++, a compara√ß√£o do movimento de centroides entre itera√ß√µes e valida√ß√£o por meio do m√©todo da silhueta e do √≠ndice de Davies-Bouldin.

#### M√©todo do cotovelo

A quantidade de clusters a serem utilizados pelo algoritmo deve ser conhecida a priori. O m√©todo do cotovelo[^9] ‚Äî _Elbow method_ ‚Äî √© uma forma de se obter esse n√∫mero com base na itera√ß√£o entre poss√≠veis centros de clusters e a soma dos quadrados das dist√¢ncias entre eles e os pontos de dados.

O m√©todo opera sob a l√≥gica de que, ao aumentar o n√∫mero de agrupamentos, ocorrer√° a diminui√ß√£o das dist√¢ncias intracluster, haja vista a maior proximidade dos pontos em rela√ß√£o aos centroides de seus respectivos agrupamentos. Em determinado momento, o valor de tal diminui√ß√£o se tornar√° marginal ‚Äî traduzido de maneira visual em gr√°fico, uma linha teria inicialmente quedas acentuadas para, em seguida, se estabilizar na posi√ß√£o horizontal, formando um "cotovelo".

<img src="666.svg" width="100%">

O ponto em que essa estabiliza√ß√£o se torna percept√≠vel representa uma estimativa do n√∫mero ideal de clusters.

Considerando-se a mera observa√ß√£o de um gr√°fico para aferi√ß√£o de resultado sobre o n√∫mero ideal de clusters, abdica-se de suporte estat√≠stico para assegurar a robustez do
m√©todo do cotovelo. Schubert [^10] apresenta o m√©todo aplicado a conjuntos de dados com
clusters mais ou menos coesos visualmente, em que os resultados se mostram semelhantes
mesmo nos conjuntos uniformes ou quando os dados cont√™m uma √∫nica distribui√ß√£o normal.
Entre os problemas associados ao gr√°fico do cotovelo est√£o a aus√™ncia de medi√ß√£o
significativa de √¢ngulo e mudan√ßa de escala dos eixos, o que pode alterar a interpreta√ß√£o
humana de um "cotovelo".

Para mitigar tais problemas poder-se-ia utilizar um m√©todo menos subjetivo, como o crit√©rio de raz√£o de vari√¢ncia ‚Äî __Variance Ratio Criterion__ [VRC]. Enquanto o m√©todo do cotovelo se apoia na soma dos quadrados das dist√¢ncias entre cada ponto e o centroide do cluster, o VRC mede a raz√£o entre a soma da dispers√£o entre os clusters e a soma da dispers√£o dentro dos clusters[^11]. Por termos um conjunto de dados que n√£o aponta para uniformidade ou distribui√ß√£o normal, optou-se pelo m√©todo do cotovelo.

#### K-Means++

A determina√ß√£o do n√∫mero de clusters, por√©m, n√£o garante que o algoritmo encontre os melhores pontos para servirem de centroides. A determina√ß√£o do n√∫mero de clusters, por√©m, n√£o garante que o algoritmo encontre os melhores pontos para servirem de centroides. Quando se utiliza a inicializa√ß√£o rand√¥mica, em que os centroides iniciais s√£o escolhidos aleatoriamente dentro do cluster, √© poss√≠vel que sejam escolhidos pontos muito pr√≥ximos uns dos outros. A alta sensibilidade da t√©cnica de agrupamento pode levar a uma solu√ß√£o de m√≠nimo local em vez de uma global, gerando parti√ß√µes que n√£o sejam ideais[^12].

Para sobrepor tal limita√ß√£o, este trabalho se utilizou do m√©todo de inicializa√ß√£o K-Means++[^13], em que o centroide passa por itera√ß√µes, e √© selecionado a partir da probabilidade de determinado ponto ser o melhor centroide com base na dist√¢ncia em rela√ß√£o aos outros pontos de dados. A mudan√ßa sucessiva entre centroides reduz as chances de o algoritmo K-Means convergir para uma solu√ß√£o abaixo do ideal.

Dado um conjunto de pontos \\(D\\) e um conjunto de centroides selecionados \\(C\\), a probabilidade de se escolher o ponto de dado \\(x\\) como pr√≥ximo centroide √© calculada por meio de

$$
P(x) = \frac{D(x)^2}{\sum_{x^{\prime} \in D}D(x^{\prime})^2}
$$

sendo, \\(D(x)\\): dist√¢ncia entre o ponto \\(x\\) e o centroide mais pr√≥ximo em \\(C\\).

Com os centroides inicializados, cada ponto √© atribu√≠do ao centroide mais pr√≥ximo. Pontos pr√≥ximos ao centroide formam clusters. Considerando o ponto \\(x\\) e um conjunto de centroides \\(C\\), o r√≥tulo do cluster \\(l\\) ao qual \\(x\\) pertence √© computado por

$$
l(x) = \arg \min_{c \in C}\Vert x - c \Vert
$$

Em seguida, cada centroide √© recalculado tomando a m√©dia da dist√¢ncia de todos os pontos a eles atribu√≠dos,

$$
c_i = \frac{1}{\vert S_i \vert}\sum_{x \in S_i} x
$$

onde, \\(S_i\\): conjunto de todos os pontos atribu√≠dos ao centroide \\(i\\).

A cada itera√ß√£o de atualiza√ß√£o de centroides √© computada a in√©rcia. Para conjunto univariado,

$$
\sum_{i=1}^{n}{\Vert {x_i} - {c_{l(x_i)}}\Vert}^2
$$

onde, \\(c\_{l(x_i)}\\): centroide do cluster para o qual \\(x_i\\) foi atribu√≠do.

#### Crit√©rios aprimorados para converg√™ncia

Al√©m da inicializa√ß√£o por K-Means++, o algoritmo adota crit√©rios de converg√™ncia avan√ßados ao comparar o movimento dos centroides entre itera√ß√µes. Sendo \\(C_t\\) o conjunto de centroides na itera√ß√£o \\(t\\), o algoritmo converge se

$$
\max_{c \in C_t}\Vert c - c_{t - 1} \Vert < tol
$$

onde,

- \\(\Vert c - c\_{t - 1} \Vert\\) dist√¢ncia euclidiana
- \\(tol\\): toler√¢ncia especificada

#### Valida√ß√£o pelo m√©todo da silhueta

A valida√ß√£o dos resultados obtidos a partir da implementa√ß√£o dessas t√©cnicas foi realizada, primeiro, pelo m√©todo da silhueta[^14] ‚Äî _Silhouette method_. Esta t√©cnica observa a similaridade de um ponto com seu cluster em compara√ß√£o com outros clusters a partir de

$$
s_i = \frac{{b_i} - {a_i}}{\max({a_i},{b_i})}
$$

onde,

- \\(a_i\\): dist√¢ncia m√©dia de \\(i\\) para todos os outros pontos intra-agrupamento
- \\(b_i\\): a menor dist√¢ncia m√©dia de \\(i\\) para todos os pontos em agrupamentos diferentes

O m√©todo da silhueta retorna resultados no intervalo de -1 a 1. Se o valor for:

- pr√≥ximo de -1: o ponto est√° agrupado de maneira errada;
- pr√≥ximo de 0: o ponto est√° entre dois clusters, de forma que o agrupamento pode ser aprimorado;
- pr√≥ximo de 1: o ponto est√° bem agrupado.

#### Valida√ß√£o pelo √≠ndice de Davies-Bouldin

Enquanto o m√©todo da silhueta faz compara√ß√£o entre um ponto √∫nico e os agrupamentos, o √≠ndice de Davies-Bouldin[^15], segunda medida usada na valida√ß√£o dos resultados, observa a coes√£o do cluster, dada a l√≥gica de que um agrupamento adequado √© denso em si, ao passo que distante dos demais agrupamentos.

Melhor o agrupamento quanto mais pr√≥ximo de 0 o √≠ndice √©, resultado obtido por

$$
\frac{1}{k}\sum_{i=1}^{k}\max_{i \ne j}\bigg(\frac{{S_i}+{S_j}}{M_{ij}}\bigg)
$$

sendo,

- \\(k\\): n√∫mero de clusters;
- \\(i\\),\\(j\\): clusters diferentes;
- \\(S_i\\), \\(S_j\\): dispers√£o interna dos clusters \\(i\\) e \\(j\\), respectivamente;
- \\(M\_{ij}\\): dist√¢ncia entre clusters \\(i\\) e \\(j\\)

## Resultados

Realizou-se uma an√°lise explorat√≥ria para compreender os dados e sua dispers√£o. No quinqu√™nio observado, foram 4.453 registros de despesas em 86 n√∫meros √∫nicos de CNPJ, totalizando R$ 1.784.601,08 ap√≥s ajuste inflacion√°rio. Cada despesa apresentou valor m√©dio de R$ 400,76, por√©m com coeficiente de varia√ß√£o de 241,41%, indicando significativa dispers√£o dos dados em rela√ß√£o √† m√©dia.

Notou-se ainda que a m√©dia √© superior ao terceiro quartil. Isso denota inclina√ß√£o de dados para valores mais baixos. O conjunto apresenta, assim, cauda √† direita mais longa do que √† esquerda, o que √© corroborado pela assimetria de 5,21, enquanto a curtose de 32,67 demonstra pico acentuado em compara√ß√£o √† distribui√ß√£o normal.

| Medida                      | Valor      |
| --------------------------- | ---------- |
| Contagem                    | 4.453      |
| M√©dia (R$)                  | 400,763773 |
| Desvio-padr√£o (R$)          | 967,469752 |
| M√≠nimo (R$)                 | 6,49       |
| 1¬∫ Quartil (R$)             | 55,75      |
| 2¬∫ Quartil (R$)             | 123,14     |
| 3¬∫ Quartil (R$)             | 276,18     |
| M√°ximo (R$)                 | 10.250,41  |
| Coeficiente de varia√ß√£o (%) | 241,40648  |
| Assimetria                  | 5,21061    |
| Curtose                     | 32,66851   |

As despesas foram agrupadas por empresa, a fim de manter o comportamento dos gastos dentro da variabilidade de valores para cada CNPJ. A presente implementa√ß√£o do algoritmo de K-Means processou as informa√ß√µes para cada estabelecimento seguindo os seguintes par√¢metros:

| Par√¢metro                           | Valor                                       |
| ----------------------------------- | ------------------------------------------- |
| N√∫mero m√≠nimo de clusters           | 2                                           |
| N√∫mero de clusters utilizados       | 2 a 10, selecionado pelo m√©todo do cotovelo |
| M√°ximo de itera√ß√µes                 | 100                                         |
| Toler√¢ncia para converg√™ncia        | 0,0001                                      |
| Percentil para detec√ß√£o de anomalia | 95                                          |

Como resultado foram obtidas 262 anomalias que somaram R$ 197.697,24 ‚Äî 11,08% do valor total de despesas. Por anomalias entendem-se padr√µes em dados que n√£o se ajustam √† no√ß√£o bem definida de comportamento normal[^16] ‚Äî no contexto deste trabalho, anomalias s√£o valores de despesas que n√£o se enquadram nos agrupamentos criados pelo algoritmo. Por defini√ß√£o, n√£o se pode tratar toda anomalia como fraude: h√° anomalias que se encontram no meio de todas as despesas de determinada empresa, n√£o sendo os maiores valores no conjunto. Tais anomalias entre clusters s√£o tratadas aqui como falsos positivos.

Dado o papel dos clusters neste algoritmo e a implementa√ß√£o de K-Means++, h√° grande variabilidade no n√∫mero de clusters. No conjunto de 86 empresas, o n√∫mero de clusters vai de 2 a 10. Validamos tais valores por meio do dois instrumentos supracitados:

1. M√©todo da silhueta, cujos resultados aceit√°veis devem estar entre 0,5 e 1 de uma escala que vai de -1 a 1;
2. √çndice de Davies-Bouldin, com resultados ideais entre 0 a 0,5, numa escala que vai de 0 a 1.

A quantidade de clusters de cada CNPJ foi validada por meio dos dois instrumentos supracitados: o m√©todo da silhueta e o √≠ndice de Davies-Bouldin. Um resultado adequado para o primeiro deles estaria entre 0,5 e 1 de uma escala de -1 a 1; o segundo, de 0 a 0,5 na escala de 0 a 1.

Do conjunto de 86 empresas, todas apresentam resultados ideais para o m√©todo da silhueta (valores entre 0,577 e 0,918); 79 apresentaram resultados ideais para o √≠ndice de Davies-Bouldin (valores entre 0,166 e 0,489), enquanto sete apresentaram resultados abaixo do ideal (valores entre 0,508 e 0,573).

Com a clusteriza√ß√£o das despesas, a detec√ß√£o de anomalias segundo o algoritmo e a valida√ß√£o dos m√©todos aplicados, foi realizada uma an√°lise final para considerar anomalias pass√≠veis de inquiri√ß√£o dos √≥rg√£os de controle aquelas cujos valores s√£o maiores que o maior valor de n√£o anomalia do √∫ltimo cluster. Com isso, descartaram-se anomalias posicionadas entre clusters, e o resultado obtido foi de 46 anomalias em 32 empresas, com valor total de R$ 44.348,88.

{{< expandable label="Veja empresas e anomalias" level="2" >}}

<div style="overflow-x:auto;width:100%;">
<table style="font-size:0.85em;table-layout:auto;border-color:black;">
<thead>
<tr>
<th>CNPJ</th>
<th>Valor original (R$)</th>
<th>Valor corrigido (R$)</th>
<th>Quantidade de clusters para o CNPJ</th>
<th>Resultado do m√©todo da silhueta</th>
<th>Resultado do √≠ndice de Davies-Bouldin</th>
</tr>
</thead>
<tbody>
<tr>
<td>02.012.862/0001-60</td>
<td>9.525,39</td>
<td>9.584,44</td>
<td>6</td>
<td>0,5996</td>
<td>0,4816</td>
</tr>
<tr>
<td>03.071.465/0001-21</td>
<td>1.340,00</td>
<td>1.658,78</td>
<td>3</td>
<td>0,6767</td>
<td>0,4664</td>
</tr>
<tr>
<td>03.300.974/0049-23</td>
<td>229,12</td>
<td>298,95</td>
<td>2</td>
<td>0,6579</td>
<td>0,4856</td>
</tr>
<tr>
<td>08.402.977/0001-47</td>
<td>266,51</td>
<td>269,26</td>
<td>4</td>
<td>0,7556</td>
<td>0,3117</td>
</tr>
<tr>
<td>09.060.964/0106-77</td>
<td>360,91</td>
<td>448,74</td>
<td>6</td>
<td>0,6681</td>
<td>0,5129</td>
</tr>
<tr>
<td>09.060.964/0106-77</td>
<td>314,57</td>
<td>389,17</td>
<td>6</td>
<td>0,6681</td>
<td>0,5129</td>
</tr>
<tr>
<td>09.399.877/0001-71</td>
<td>1.398,26</td>
<td>1.788,63</td>
<td>4</td>
<td>0,6203</td>
<td>0,5162</td>
</tr>
<tr>
<td>09.438.123/0001-83</td>
<td>445,86</td>
<td>570,85</td>
<td>3</td>
<td>0,6277</td>
<td>0,5329</td>
</tr>
<tr>
<td>09.456.178/0001-16</td>
<td>229,75</td>
<td>285,66</td>
<td>4</td>
<td>0,6632</td>
<td>0,3914</td>
</tr>
<tr>
<td>09.456.550/0001-94</td>
<td>379,80</td>
<td>487,44</td>
<td>3</td>
<td>0,6776</td>
<td>0,4350</td>
</tr>
<tr>
<td>09.456.550/0001-94</td>
<td>354,59</td>
<td>453,99</td>
<td>3</td>
<td>0,6776</td>
<td>0,4350</td>
</tr>
<tr>
<td>09.456.704/0001-48</td>
<td>432,16</td>
<td>438,34</td>
<td>4</td>
<td>0,6629</td>
<td>0,4534</td>
</tr>
<tr>
<td>09.456.704/0001-48</td>
<td>326,36</td>
<td>405,21</td>
<td>4</td>
<td>0,6629</td>
<td>0,4534</td>
</tr>
<tr>
<td>09.456.714/0001-83</td>
<td>458,39</td>
<td>567,66</td>
<td>4</td>
<td>0,6824</td>
<td>0,4745</td>
</tr>
<tr>
<td>09.536.662/0001-55</td>
<td>403,31</td>
<td>407,22</td>
<td>3</td>
<td>0,7288</td>
<td>0,3667</td>
</tr>
<tr>
<td>11.384.785/0001-60</td>
<td>678,58</td>
<td>840,34</td>
<td>3</td>
<td>0,6506</td>
<td>0,4524</td>
</tr>
<tr>
<td>13.232.868/0001-69</td>
<td>1.360,75</td>
<td>1.683,45</td>
<td>3</td>
<td>0,6969</td>
<td>0,4445</td>
</tr>
<tr>
<td>13.232.868/0001-69</td>
<td>1.209,82</td>
<td>1.498,23</td>
<td>3</td>
<td>0,6969</td>
<td>0,4445</td>
</tr>
<tr>
<td>42.591.651/0612-82</td>
<td>110,60</td>
<td>134,45</td>
<td>6</td>
<td>0,6872</td>
<td>0,3487</td>
</tr>
<tr>
<td>42.591.651/0612-82</td>
<td>118,80</td>
<td>119,93</td>
<td>6</td>
<td>0,6872</td>
<td>0,3487</td>
</tr>
<tr>
<td>43.386.903/0001-65</td>
<td>1.361,20</td>
<td>1.361,20</td>
<td>2</td>
<td>0,9177</td>
<td>0,2157</td>
</tr>
<tr>
<td>43.386.903/0001-65</td>
<td>1.030,60</td>
<td>1.036,99</td>
<td>2</td>
<td>0,9177</td>
<td>0,2157</td>
</tr>
<tr>
<td>43.386.903/0001-65</td>
<td>249,27</td>
<td>308,69</td>
<td>2</td>
<td>0,9177</td>
<td>0,2157</td>
</tr>
<tr>
<td>44.993.632/0001-79</td>
<td>2.004,54</td>
<td>2.621,23</td>
<td>6</td>
<td>0,6270</td>
<td>0,4621</td>
</tr>
<tr>
<td>44.993.632/0001-79</td>
<td>1.700,39</td>
<td>2.218,63</td>
<td>6</td>
<td>0,6270</td>
<td>0,4621</td>
</tr>
<tr>
<td>44.993.632/0001-79</td>
<td>1.441,83</td>
<td>1.887,10</td>
<td>6</td>
<td>0,6270</td>
<td>0,4621</td>
</tr>
<tr>
<td>45.007.937/0001-27</td>
<td>1.189,20</td>
<td>1.556,45</td>
<td>5</td>
<td>0,7601</td>
<td>0,3129</td>
</tr>
<tr>
<td>47.079.637/0001-89</td>
<td>1.800,00</td>
<td>1.805,09</td>
<td>2</td>
<td>0,7795</td>
<td>0,4130</td>
</tr>
<tr>
<td>49.967.557/0001-95</td>
<td>1.395,16</td>
<td>1.777,74</td>
<td>4</td>
<td>0,7310</td>
<td>0,3074</td>
</tr>
<tr>
<td>50.244.235/0001-05</td>
<td>93,50</td>
<td>108,86</td>
<td>3</td>
<td>0,7979</td>
<td>0,2713</td>
</tr>
<tr>
<td>51.483.956/0001-22</td>
<td>140,59</td>
<td>184,01</td>
<td>3</td>
<td>0,6680</td>
<td>0,4331</td>
</tr>
<tr>
<td>54.867.247/0001-39</td>
<td>361,15</td>
<td>447,56</td>
<td>4</td>
<td>0,6375</td>
<td>0,4426</td>
</tr>
<tr>
<td>54.867.247/0001-39</td>
<td>336,96</td>
<td>359,06</td>
<td>4</td>
<td>0,6375</td>
<td>0,4426</td>
</tr>
<tr>
<td>54.867.247/0001-39</td>
<td>174,04</td>
<td>216,09</td>
<td>4</td>
<td>0,6375</td>
<td>0,4426</td>
</tr>
<tr>
<td>54.951.561/0001-03</td>
<td>236,00</td>
<td>239,37</td>
<td>8</td>
<td>0,6219</td>
<td>0,4354</td>
</tr>
<tr>
<td>56.007.859/0001-87</td>
<td>453,85</td>
<td>593,48</td>
<td>3</td>
<td>0,8057</td>
<td>0,3859</td>
</tr>
<tr>
<td>58.699.232/0001-60</td>
<td>168,16</td>
<td>218,54</td>
<td>5</td>
<td>0,6550</td>
<td>0,4581</td>
</tr>
<tr>
<td>61.084.018/0001-03</td>
<td>1.073,17</td>
<td>1.372,78</td>
<td>4</td>
<td>0,6369</td>
<td>0,4892</td>
</tr>
<tr>
<td>61.359.691/0001-09</td>
<td>180,10</td>
<td>181,82</td>
<td>5</td>
<td>0,5769</td>
<td>0,5270</td>
</tr>
<tr>
<td>61.563.557/0001-25</td>
<td>238,45</td>
<td>242,33</td>
<td>4</td>
<td>0,7763</td>
<td>0,3427</td>
</tr>
<tr>
<td>61.980.272/0012-42</td>
<td>172,88</td>
<td>219,43</td>
<td>3</td>
<td>0,7751</td>
<td>0,4507</td>
</tr>
<tr>
<td>65.684.037/0003-93</td>
<td>636,78</td>
<td>790,71</td>
<td>5</td>
<td>0,6320</td>
<td>0,4574</td>
</tr>
<tr>
<td>65.684.037/0003-93</td>
<td>513,97</td>
<td>647,51</td>
<td>5</td>
<td>0,6320</td>
<td>0,4574</td>
</tr>
<tr>
<td>65.684.037/0003-93</td>
<td>422,30</td>
<td>525,07</td>
<td>5</td>
<td>0,6320</td>
<td>0,4574</td>
</tr>
<tr>
<td>65.684.037/0003-93</td>
<td>399,87</td>
<td>495,19</td>
<td>5</td>
<td>0,6320</td>
<td>0,4574</td>
</tr>
<tr>
<td>66.728.858/0001-85</td>
<td>482,40</td>
<td>603,21</td>
<td>7</td>
<td>0,6492</td>
<td>0,4156</td>
</tr>
</tbody>
</table>
</div>
{{< /expandable >}}

## C√≥digos comentados

### Algoritmo

```py
from typing import Tuple
import numpy as np


class KMeans:
    """
    k-means com crit√©rios de converg√™ncia aprimorados.

    Atributos:
        k (int): N√∫mero de clusters.
        max_iters (int): N√∫mero m√°ximo de itera√ß√µes para o k-means.
        tol (float): Toler√¢ncia de converg√™ncia baseada no movimento do
            centroide.
        n_init (int): N√∫mero de vezes que o algoritmo ser√° executado com
            diferentes seeds de centroides.
        threshold (int): Percentil para detec√ß√£o de anomalias.
        centroids (np.ndarray): Centroides para os clusters.
    """

    def __init__(
        self,
        k: int = 2,
        max_iters: int = 100,
        tol: float = 1e-4,
        n_init: int = 30,
        threshold: int = 95,
        centroids: np.ndarray = None,
    ):
        """
        Inicializa√ß√£o com par√¢metros especificados.
        """
        self.k = k
        self.max_iters = max_iters
        self.tol = tol
        self.n_init = n_init
        self.threshold = threshold
        self.centroids = centroids

    @staticmethod
    def _kpp_init(data: np.ndarray, k: int) -> np.ndarray:
        """
        Inicializa os centroides usando o m√©todo k-means++.

        Argumentos:
            data (np.ndarray): Dados de entrada.
            k (int): N√∫mero de centroides desejados.

        Retorna:
            centroids (np.ndarray): Centroides inicializados.
        """
        # seleciona ponto aleat√≥rio como centroide
        centroids = [data[np.random.choice(len(data))]]

        # itera sobre centroides restantes
        for _ in range(1, k):
            # calcula o quadrado da dist√¢ncia entre cada ponto e o
            # centroide mais pr√≥ximo
            squared_dist = np.array(
                [np.min([np.linalg.norm(c - x) ** 2 for c in centroids]) for x in data]
            )
            # calcula a probabilidade de selecionar cada ponto de dado
            # como novo centroide
            probs = squared_dist / squared_dist.sum()
            # escolhe o ponto com maior probabilidade como novo
            # centroide
            centroid = data[np.argmax(probs)]
            # adiciona novo centroide √† lista de centroides
            centroids.append(centroid)
        return np.array(centroids)

    def get_optimal_k(self, data: np.ndarray, k_max: int = 10) -> int:
        """
        Aplica m√©todo Elbow para obter o n√∫mero de clusters ideal.

        Argumentos:
            data (np.ndarray): Dados usados no algoritmo K-Means.
            k_max (int): N√∫mero m√°ximo de clusters. Valor-padr√£o: 10.

        Retorna:
            optimal_k (int): N√∫mero de clusters ideal.
        """
        # lista para armazenar in√©rcia de cada k
        sum_sq = []
        # itera sobre intervalo de 1 a 10
        for k in range(1, k_max + 1):
            # ajusta o n√∫mero de clusters para a itera√ß√£o atual
            self.k = k
            # ajusta os dados ao algoritmo
            self.fit(data)
            # calcula a in√©rcia
            inertia = np.sum(
                [
                    np.linalg.norm(data[i] - self.centroids[self.labels[i]]) ** 2
                    for i in range(len(data))
                ]
            )
            # adiciona a in√©rcia √† lista
            sum_sq.append(inertia)
        # calcula a diferen√ßa dos valores de in√©rcia para encontrar o
        # cotovelo
        diffs = np.diff(sum_sq, 2)
        # escolhe k ideal a partir da menor diferen√ßa
        optimal_k = np.argmin(diffs) + 1
        return optimal_k

    def _single_run(self, data: np.ndarray) -> Tuple[np.ndarray, np.ndarray, float]:
        """
        Realiza execu√ß√£o √∫nica do algoritmo k-means.

        Argumentos:
            data (np.ndarray): Dados de entrada.

        Retorna:
            centroids (np.ndarray): Melhores centroides ap√≥s a execu√ß√£o
                do k-means.
            labels (np.ndarray): Atribui√ß√µes de cluster para cada ponto
                de dado.
            inertia (float): Dist√¢ncia total dos pontos de dados a
                partir de seus centroides atribu√≠dos.
        """
        # inicializa centoides
        centroids = self._kpp_init(data, self.k)

        # itera sobre max_iters:
        for _ in range(self.max_iters):
            # calcula a dist√¢ncia entre cada ponto e cada centroide
            dist = np.linalg.norm(data[:, np.newaxis] - centroids, axis=2)
            # atribui cada ponto ao centroide mais pr√≥ximo
            labels = np.argmin(dist, axis=1)
            # calcula os novos centroides com base na atribui√ß√£o recente
            new_centroids = np.array(
                [data[labels == i].mean(axis=0) for i in range(self.k)]
            )
            # observa se a mudan√ßa no centroide est√° abaixo da
            # toler√¢ncia
            if np.all(np.abs(new_centroids - centroids) < self.tol):
                # interrompe a itera√ß√£o
                break
            # sobrescreve lista de centroides
            centroids = new_centroids
        # calcula a in√©rcia
        inertia = np.sum(
            [
                np.linalg.norm(data[i] - centroids[labels[i]]) ** 2
                for i in range(len(data))
            ]
        )
        return centroids, labels, inertia

    def fit(self, data: np.ndarray) -> None:
        """
        Ajusta o algoritmo k-means aos dados.

        Argumento:
            data (np.ndarray): Dados de entrada.
        """
        # atribui valor infinito √† in√©rcia m√≠nima
        min_inertia = float("inf")
        # atribui None aos melhores centroides
        best_centroids = None
        # atribui None √†s melhores labels
        best_labels = None

        # itera sobre quantidade de execu√ß√µes de K-Means
        for _ in range(self.n_init):
            # obt√©m valores de centroides, labels, in√©cia
            centroids, labels, inertia = self._single_run(data)
            # observa se a execu√ß√£o atual tem menor in√©rcia
            if inertia < min_inertia:
                # atualiza in√©rcia m√≠nima
                min_inertia = inertia
                # atualiza melhores centroides
                best_centroids = centroids
                # atualiza melhores labels
                best_labels = labels

        # ajusta os valores de centroides para os melhores valores
        # encontrados
        self.centroids = best_centroids
        # ajusta os valores de labels para os melhores valores
        # encontrados
        self.labels = best_labels

    def detect(self, data: np.ndarray) -> np.ndarray:
        """
        Detecta anomalias nos dados com base na dist√¢ncia ao centroide
        mais pr√≥ximo.

        Argumentos:
            data (np.ndarray): Dados de entrada.

        Retorna:
            anomalies (np.ndarray): Anomalias detectadas.
        """
        # calcula a dist√¢ncia entre cada ponto e o centroide mais
        # pr√≥ximo
        dist = np.min(
            np.linalg.norm(data[:, np.newaxis] - self.centroids, axis=2), axis=1
        )
        # ajusta o limite com base no percentil inserido
        threshold = np.percentile(dist, self.threshold)
        # considera anomalias os pontos cujas dist√¢ncias s√£o maiores que
        # o limite
        anomalies = data[dist > threshold]
        return anomalies

    def get_labels(self, data: np.ndarray) -> np.ndarray:
        """
        Atribui cada ponto de dado ao centroide mais pr√≥ximo para
        determinar seu cluster.

        Argumento:
            data (np.ndarray): Conjunto de dados.

        Retorna:
            labels (np.ndarray): Array de labels de cluster
                correspondentes a cada ponto de dado.
        """
        # calcula a dist√¢ncia de cada ponto a cada centroide
        dist = np.linalg.norm(data[:, np.newaxis] - self.centroids, axis=2)
        # atribui cada ponto ao centroide mais pr√≥ximo
        labels = np.argmin(dist, axis=1)
        return labels


class Score:
    """
    C√°lculo de scoring para algoritmo de clusteriza√ß√£o.
    """

    @staticmethod
    def silhouette(data: np.ndarray, labels: np.ndarray) -> float:
        """
        Calcula o score do m√©todo da silhueta.

        Argumentos:
            data (np.ndarray): Dados de entrada.
            labels (np.ndarray): Atribui√ß√µes de cluster para cada ponto
                de dado.

        Retorna:
            float: valor do m√©todo da silhueta.
        """
        # obt√©m labels √∫nicas
        unique_labels = np.unique(labels)
        # lista para armazenar valores do m√©todo da silhueta
        silhouette_vals = []
        # itera sobre pontos de dados
        for index, label in enumerate(labels):
            # obt√©m pontos que est√£o no mesmo cluster
            same_cluster = data[labels == label]
            # calcula a dist√¢ncia m√©dia a outros pontos no mesmo cluster
            a = np.mean(np.linalg.norm(same_cluster - data[index], axis=1))
            # extrai pontos de outros clusters
            other_clusters = [
                data[labels == other_label]
                for other_label in unique_labels
                if other_label != label
            ]
            # calcula a dist√¢ncia m√©dia para pontos em outros clusters
            b_vals = [
                np.mean(np.linalg.norm(cluster - data[index], axis=1))
                for cluster in other_clusters
            ]
            # obt√©m os menores valores
            b = min(b_vals)
            # calcula o valor da silhueta
            silhouette_vals.append((b - a) / max(a, b))
        # retorna a silhueta m√©dia para todos os pontos
        return np.mean(silhouette_vals)

    @staticmethod
    def daviesbouldin(data: np.ndarray, labels: np.ndarray) -> float:
        """
        Calcula o √≠ndice de Davies-Bouldin.

        Argumentos:
            data (np.ndarray): Dados de entrada.
            labels (np.ndarray): Atribui√ß√µes de cluster para cada ponto
                de dado.

        Retorna:
            float: valor de Davies-Bouldin calculado.
        """
        # obt√©m labels √∫nicas
        unique_labels = np.unique(labels)
        # calcula o centroide para cada cluster
        centroids = np.array(
            [data[labels == label].mean(axis=0) for label in unique_labels]
        )
        # calcula a dist√¢ncia m√©dia dentro de cada cluster
        avg_dist_within_cluster = np.array(
            [
                np.mean(
                    np.linalg.norm(data[labels == label] - centroids[label], axis=1)
                )
                for label in unique_labels
            ]
        )
        # calcula a dist√¢ncia entre centroides
        centroid_dist = np.linalg.norm(centroids[:, np.newaxis] - centroids, axis=2)
        # ajusta valores diagonais para infinito
        np.fill_diagonal(centroid_dist, float("inf"))
        # calcula a raz√£o entre a soma das dist√¢ncias m√©dias e a
        # dist√¢ncia entre centroides
        cluster_ratios = (
            avg_dist_within_cluster[:, np.newaxis] + avg_dist_within_cluster
        ) / centroid_dist
        # obt√©m a maior raz√£o para cada cluster
        max_cluster_ratios = np.max(cluster_ratios, axis=1)
        # retorna a m√©dia das maiores raz√µes
        return np.mean(max_cluster_ratios)
```

### Execu√ß√£o

```py
import os
import asyncio
import glob
from typing import List, Dict, Union
from itertools import groupby
import xml.etree.ElementTree as ET
import aiohttp
from aiolimiter import AsyncLimiter
import pandas as pd
import numpy as np
import sys

sys.path.insert(0, "..")
from src.kmeans import KMeans, Score


async def download_xml(year: int, semaphore: asyncio.Semaphore) -> None:
    """
    Realiza download ass√≠ncrono de xml para um √∫nico ano.

    Argumentos:
        year (int): Ano do arquivo xml.
        semaphore (asyncio.Semaphore): Controlador de acesso concorrente.
    """
    limiter = AsyncLimiter(1, 0.125)
    USER_AGENT = ""
    headers = {"User-Agent": USER_AGENT}
    DATA_DIR = os.path.join(os.getcwd(), "../data")
    if not os.path.exists(DATA_DIR):
        os.mkdir(DATA_DIR)
    url = f"https://www.al.sp.gov.br/repositorioDados/deputados/despesas_gabinetes_{str(year)}.xml"
    async with aiohttp.ClientSession(headers=headers) as session:
        await semaphore.acquire()
        async with limiter:
            async with session.get(url) as resp:
                content = await resp.read()
                semaphore.release()
                file = f"despesas_gabinetes_{str(year)}.xml"
                with open(os.path.join(DATA_DIR, file), "wb") as f:
                    f.write(content)


async def fetch_expenses(year_start: int, year_end: int) -> None:
    """
    Realiza download ass√≠ncrono de xml para um per√≠odo.

    Argumentos:
        year_start (int): In√≠cio do per√≠odo.
        year_end (int): Fim do per√≠odo.
    """
    tasks = set()
    semaphore = asyncio.Semaphore(value=10)
    for i in range(int(year_start), int(year_end) + 1):
        task = asyncio.create_task(download_xml(i, semaphore))
        tasks.add(task)
    await asyncio.wait(tasks, return_when=asyncio.ALL_COMPLETED)


def parse_data(list_files: List[str]) -> List[Dict[str, Union[str, None]]]:
    """
    Interpreta dados dos arquivos xml e extrai informa√ß√µes relevantes.

    Argumentos:
        list_files (list): Lista dos caminhos para os arquivos xml.

    Retorna:
        data (list): Lista de dicion√°rios de despesas.
    """
    data = list()
    for file in list_files:
        tree = ET.parse(file)
        xroot = tree.getroot()
        for child in xroot.iter("despesa"):
            cols = [elem.tag for elem in child]
            values = [elem.text for elem in child]
            data.append(dict(zip(cols, values)))
    return data


# executa `fetch_expenses` no per√≠odo de 2013 a 2022
asyncio.run(fetch_expenses(2013, 2022))
# observa se h√° o diret√≥rio `data`
if os.path.exists(os.path.join(os.getcwd(), "../data")):
    # acessa diret√≥rio
    os.chdir("../data")
    # lista arquivos xml
    files = glob.glob("*.xml")
    # interpreta os arquivos
    load = parse_data(files)
    # armazena os dados na vari√°vel `despesas`
    despesas = pd.DataFrame.from_dict(load)
# leitura dos data de IPCA
ipca = pd.read_csv("../data/ipca.csv")
# convers√£o da vari√°vel Data para datetime
ipca["Data"] = pd.to_datetime(ipca["Data"])
# parseamento da data
despesas["Data"] = pd.to_datetime(
    despesas["Ano"].astype(str) + (despesas["Mes"].astype(str)).str.zfill(2) + "01"
)
# filtro da categoria de despesa
despesas = despesas[
    despesas["Tipo"] == "I - HOSPEDAGEM, ALIMENTA√á√ÉO E DESPESAS DE LOCOMO√á√ÉO"
]
# manuten√ß√£o das colunas estritamente necess√°rias
despesas = despesas[["Data", "CNPJ", "Valor"]]
# filtro a partir de 2018
despesas = despesas[despesas["Data"].dt.year > 2017]
# jun√ß√£o das duas bases
data = pd.merge(left=despesas, right=ipca, on="Data", how="inner")
# ajuste para o valor de dezembro de 2022
data["Valor_ref"] = ipca[ipca["Data"] == "2022-12-01"]["Valor"].values[0]
# c√°lculo da defla√ß√£o
data["Valor_corrigido"] = round(
    (data["Valor_ref"].astype(float) / data["Valor_y"].astype(float))
    * data["Valor_x"].astype(float),
    2,
)
# remo√ß√£o de vari√°veis desnecess√°rias
data = data[["CNPJ", "Valor_corrigido"]]
# remo√ß√£o de linhas com CNPJ nulos
data = data[data["CNPJ"].notnull()]
# filtro para CNPJs com apenas >= 20 entradas
data = data.groupby("CNPJ").filter(lambda x: len(x) >= 20)
# cria√ß√£o de listas para comportar os valores do m√©todo de silhueta e
# √≠ndice de Davies-Bouldin
sils, dbs = list(), list()
# inicializa√ß√£o do algoritmo de K-Means
kmeans = KMeans()
# organiza√ß√£o dos dados
selecao_dados = sorted(zip(data["CNPJ"], data["Valor_corrigido"]), key=lambda x: x[0])
# lista vazia para resultados finais
resultados_lista = []

# itera√ß√£o por CNPJ e cole√ß√£o de despesas
for cnpj, grupo in groupby(selecao_dados, key=lambda x: x[0]):
    # lista vazia de centroides
    centroids_list = []
    # convers√£o para array
    values = np.array([item[1] for item in grupo])
    # obten√ß√£o do k ideal
    kmeans.k = kmeans.get_optimal_k(values.reshape(-1, 1))
    # ajuste de dados ao algoritmo
    kmeans.fit(values.reshape(-1, 1))
    # detec√ß√£o de anomalias
    anomalies_kmeans = kmeans.detect(values.reshape(-1, 1))
    # c√°lculo do m√©todo de silhueta
    silhouette_score = Score.silhouette(
        values.reshape(-1, 1), kmeans.get_labels(values.reshape(-1, 1))
    )
    # c√°lculo do √≠ndice de Davies-Bouldin
    db_score = Score.daviesbouldin(
        values.reshape(-1, 1), kmeans.get_labels(values.reshape(-1, 1))
    )
    # obten√ß√£o de labels
    labels = kmeans.get_labels(values.reshape(-1, 1))
    # itera√ß√£o sobre labels e valores
    for value, label in zip(values, labels):
        # adi√ß√£o de label no dicion√°rio
        centroids_list.append({"centroid": kmeans.centroids[label][0]})
    # contador zerado
    centroid_idx = 0
    # itera√ß√£o sobre despesas
    for value in values:
        # atribui√ß√£o de 1 para anomalia, 0 para n√£o anomalia
        is_anomaly = 1 if value in anomalies_kmeans else 0
        # adi√ß√£o de dicion√°rio na lista final
        resultados_lista.append(
            {
                "CNPJ": cnpj,
                "Valor": value,
                "Anomalia": is_anomaly,
                "Centroide": centroids_list[centroid_idx]["centroid"],
                "Clusters": kmeans.k,
                "Silhueta": silhouette_score,
                "Davies_Bouldin": db_score,
            }
        )
        # incremento do contador
        centroid_idx += 1
        
# convers√£o dos resultados em dataframe
resultados = pd.DataFrame(resultados_lista)
# salvamento como csv
resultados.to_csv("../prd/resultado.csv", index=False, encoding="utf-8")
```

## Refer√™ncias

[^1]: Assembleia Legislativa do Estado de S√£o Paulo [Alesp]. 1997. Resolu√ß√£o n. 783, de 1¬∞ de julho de 1997. Altera a Resolu√ß√£o n¬∞ 776, de 14/10/1996, que implantou a nova estrutura administrativa, cria o N√∫cleo de Qualidade e institui a verba de gabinete. Dispon√≠vel em: https://www.al.sp.gov.br/repositorio/legislacao/resolucao.alesp/1997/original-resolucao.alesp-783-01.07.1997.html. Acesso em: 19 mar√ßo 2023.
[^2]: Secretaria da Fazenda e Planejamento do Governo do Estado de S√£o Paulo. 2023. √çndices. Dispon√≠vel em: https://portal.fazenda.sp.gov.br/Paginas/Indices.aspx. Acesso em: 26 mar√ßo 2023.
[^3]: Secretaria da Fazenda e Planejamento do Governo do Estado de S√£o Paulo. 2023. Execu√ß√£o or√ßament√°ria e financeira. Dispon√≠vel em: https://www.fazenda.sp.gov.br/sigeolei131/paginas/flexconsdespesa.aspx. Acesso em: 19 mar√ßo 2023.
[^4]: Minist√©rio P√∫blico de S√£o Paulo. 2022. Sistema Eletr√¥nico de Informa√ß√µes. Dispon√≠vel em: https://www.mpsp.mp.br/sei-sistema-eletronico-de-informacoes Acesso em: 26 mar√ßo 2023.
[^5]: Tribunal de Justi√ßa do Estado de S√£o Paulo. 2023. E-SAJ. Dispon√≠vel em: https://esaj.tjsp.jus.br/esaj/portal.do?servico=190090 Acesso em: 24 setembro 2023.
[^6]: Assembleia Legislativa do Estado de S√£o Paulo. 2023. Portal de Dados Abertos. Dispon√≠vel em: https://www.al.sp.gov.br/dados-abertos/recurso/21 Acesso em: 26 mar√ßo 2023.
[^7]: Instituto Brasileiro de Geografia e Estat√≠stica. IPCA. Dispon√≠vel em: https://www.ibge.gov.br/estatisticas/economicas/precos-e-custos/9256-indice-nacional-de-precos-ao-consumidor-amplo.html?=&t=series-historicas Acesso em: 26 mar√ßo 2023.
[^8]: MacQueen, J. 1967. Some methods for classification and analysis of multivariate observations. In: 5th Berkeley Symposium on Mathematical Statistics and Probability, 1967, Los Angeles, LA, Estados Unidos, Anais‚Ä¶ p. 281-297.
[^9]: Joshi, K.D.; Nalwade, P.S. 2012. Modified K-Means for better initial cluster centres. International Journal of Computer Science and Mobile Computing 7: 219-223.
[^10]: Schubert, E. 2023. Stop using the elbow criterion for k-means and how to choose the number of clusters instead. SIGKDD Explorations Newsletter 25: 36-42.
[^11]: Cali≈Ñski, T.; Harabasz, J. 1974. A dendrite method for cluster analysis. Communications in Statistics 3: 1-27.
[^12]: Morissette, L.; Chartier, S. 2013. The K-Means clustering technique: General considerations and implementation in Mathematica. Tutorials in Quantitative Methods for Psychology 9: 15-24.
[^13]: Arthur, D.; Vassilvitskii, S. 2007. K-Means++: The advantages of careful seeding. Proceedings of Annual ACM-SIAM Symposium on Discrete Algorithms: 1027-1035.
[^14]: Rousseeuw, P.J. 1987. Silhouettes: A graphical aid to the interpretation and validation of cluster analysis. Journal of Computational and Applied Mathematics 20: 53-65.
[^15]: Davies, D.L.; Bouldin, D.W. 1979. A cluster separation measure. IEEE Transactions on Pattern Analysis and Machine Intelligence 2: 224‚Äì227.
[^16]: Chandola, V; Banerjee, A.; Kumar, V. 2009. Anomaly detection: a survey. Association for Computing Machinery Computing Surveys 41: 1-58.
