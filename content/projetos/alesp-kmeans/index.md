---
title: "Detec√ß√£o de anomalias em gastos dos deputados estaduais com K-Means"
date: 2023-09-01
toc: true
---

{{< warning >}}
O texto apresentado √© uma vers√£o resumida e editada do meu trabalho de conclus√£o de curso do MBA em Data Science e Analytics na Esalq-USP.
{{< /warning >}}

## Introdu√ß√£o

Cada um dos 94 parlamentares da Assembleia Legislativa do Estado de S√£o Paulo [Alesp] tem direito aos Aux√≠lio-Encargos Gerais de Gabinete de Deputado e Aux√≠lio-Hospedagem, referenciados conjuntamente como "verba de gabinete". Tal direito foi conferido pela resolu√ß√£o 783, artigo 11, de 1¬∫ de julho de 1997[^1], e se trata de um valor mensal devido pelo Estado aos deputados a fim de que eles possam cobrir despesas inerentes ao pleno exerc√≠cio das atividades parlamentares. A resolu√ß√£o estipula o limite m√°ximo da verba de gabinete em 1.250 unidades fiscais do Estado de S√£o Paulo [Ufesp]. Em 2022, com o valor da Ufesp em R$ 31,97[^2], o limite mensal da verba de gabinete que poderia ser ressarcido por deputado foi de R$ 39.962,50.

Naquele ano, o valor total empenhado para custeio da verba de gabinete perfez R$ 26.652.243,51[^3]. O montante foi 24,43% maior que a soma em 2021, de R$ 21.419.316,88[^3], e menor do que o valor anotado na rubrica para 2023, de R$ 28.607.099,96[^3]. Caso este montante se cumpra neste ano, ser√° a primeira vez que o valor ultrapassa R$ 28,5 milh√µes desde 2018.

Tais somas de recursos p√∫blicos podem servir, ainda que parcialmente, para infringir a lei. Um exemplo √© o processo investigat√≥rio SEI 29.0001.0246360.2021-54[^4], que apura o pagamento por loca√ß√£o de im√≥veis pertencentes a aliados pol√≠ticos do deputado Murilo Felix e nunca utilizados. Outro exemplo √© a a√ß√£o penal 0037174-14.2021.8.26.0000[^5], que apura, entre outros elementos, o ressarcimento de despesas nunca efetuadas por parte do deputado Rog√©rio Nogueira.

Com este contexto, este projeto busca ser um instrumento para avalia√ß√£o de malversa√ß√£o de dinheiro p√∫blico por meio de *unsupervised machine learning*. Seu objetivo n√£o √© afirmar peremptoriamente se determinada despesa √© fraudulenta ou n√£o; seu escopo √© servir de ferramenta para uma observa√ß√£o inicial dos gastos, que podem ser analisados por meio de clusteriza√ß√£o, onde se objetiva encontrar um grupo de despesas cujos valores s√£o an√¥malos.

## M√©todo

### An√°lise explorat√≥ria

A primeira etapa consistiu na captura dos dados a partir do Portal de Dados Abertos da Alesp[^6], onde est√£o dispon√≠veis arquivos no formato `xml` que datam desde 2002 e cont√™m elementos que indicam o per√≠odo de refer√™ncia ("Ano", "M√™s"), al√©m de informa√ß√µes tanto do parlamentar ("Matr√≠cula", "Deputado") quanto da despesa ("Fornecedor", "CNPJ", "Tipo", "Valor"). Para este trabalho, foram utilizados apenas "CNPJ" e "Valor", a fim de desconsiderar eventuais vieses ideol√≥gicos. Dado o contexto temporal dos gastos, "Ano" e "M√™s" foram usados t√£o somente para realizar a defla√ß√£o dos valores at√© 31 de dezembro de 2022 seguindo o √≠ndice de pre√ßo ao consumidor amplo [IPCA], conforme divulgado pelo Instituto Brasileiro de Geografia e Estat√≠stica [IBGE][^7]. Com isso, descartou-se a temporalidade das despesas.

Foram inseridas neste estudo apenas as despesas relacionadas a alimenta√ß√£o e hospedagem compreendidas entre os anos de 2018 e 2022. Descartaram-se, ainda, fornecedores com menos de 20 despesas no quinqu√™nio, haja vista a necessidade de se ter n√∫mero significativo para a realiza√ß√£o de clusteriza√ß√£o.

Uma an√°lise explorat√≥ria foi realizada para compreender os dados e sua dispers√£o no conjunto. No quinqu√™nio observado, foram 4.453 registros de despesas em 86 n√∫meros de CNPJ √∫nicos, totalizando R$ 1.784.601,08. Cada despesa apresentou valor m√©dio de R$ 400,46, por√©m com desvio-padr√£o elevado (R$ 967,47), indicando significativa dispers√£o dos dados em rela√ß√£o √† m√©dia. O coeficiente de varia√ß√£o de 241,41% demonstrou alto grau de variabilidade relativo √† m√©dia.

Notou-se ainda que a m√©dia √© superior ao terceiro quartil. Isso indica que o conjunto de dados est√° inclinado para valores mais baixos, apesar da significante presen√ßa de outliers que puxa o terceiro quartil para cima. Graficamente, o valor m√©dio maior que o terceiro quartil sugere assimetria positiva: a cauda do lado direito √© mais longa do que do lado esquerdo. Essa indica√ß√£o √© corroborada com a assimetria de 5,21, enquanto a curtose de 32,67 comprova cauda longa e picos acentuados em compara√ß√£o √† distribui√ß√£o normal.

| Medida | Valor |
|---|---|
| Contagem | 4.453 |
| M√©dia (R$) | 400,763773 |
| Desvio-padr√£o (R$) | 967,469752 | 
| M√≠nimo (R$) | 6,49 | 
| 1¬∫ Quartil (R$) | 55,75 | 
| 2¬∫ Quartil (R$) | 123,14 | 
| 3¬∫ Quartil (R$) | 276,18 |
| M√°ximo (R$) | 10.250,41 |
| Coeficiente de varia√ß√£o (%) | 241,40648... |
| Assimetria | 5,21061... |
| Curtose | 32,66851... |

### Algoritmo de K-Means

Em seguida, foi constru√≠do um algoritmo de clusteriza√ß√£o por K-Means. Em linhas gerais, K-Means √© um algoritmo que particiona um conjunto de pontos de dados em clusters n√£o sobrepostos, sendo pr√©-determinada a quantidade de clusters[^8]. Cada ponto de dado pertence ao cluster com a menor dist√¢ncia m√©dia entre ele e um centro (centroide). 

Dado um conjunto de observa√ß√µes \\(x = \lbrace x_1, x_2, ..., x_n\rbrace \\), o algoritmo reparte as \\(n\\) observa√ß√µes em \\(k \(\geq n\)\\) conjuntos \\(S = \lbrace S_1, S_2, ..., S_k \rbrace\\) a fim de minimizar a soma dos quadrados dentro do cluster.

$$
\sum_{i = 1}^{k}\sum_{x \in S_i}{\Vert x - \mu_i \Vert}^2
$$

onde,

- \\(k\\): n√∫mero de clusters
- \\(ùëÜ_ùëñ\\): cluster \\(i\\)
- \\(ùë•\\): ponto de dado
- \\(\mu_i\\): m√©dia da dist√¢ncia dos pontos em \\(ùëÜ_ùëñ\\)

Considerando que o conjunto de dados deste trabalho √© univariado e o algoritmo aplicado visa encontrar anomalias, visualmente temos:

1. Os pontos s√£o distribu√≠dos conforme seus valores:<br><img src="111.svg" width="100%">
2. Com a quantidade de clusters pr√©-determinada, s√£o calculados os centroides a partir da minimiza√ß√£o do quadrado das dist√¢ncias:<br><img src="222.svg" width="100%">
3. Os pontos pr√≥ximos aos centroides formam clusters:<br><img src="333.svg" width="100%">.
4. Os pontos que n√£o se encontram nos clusters s√£o considerados anomalias:<br><img src="444.svg" width="100%">.

A aplica√ß√£o de K-Means, por√©m, imp√µe algumas necessidades a este trabalho:

- Pr√©-determina√ß√£o da quantidade de clusters;
- Inicializa√ß√£o de centroides considerando m√≠nimo global em vez de m√≠nimo local;
- Crit√©rio para converg√™ncia ideal dos centroides;
- Valida√ß√£o dos resultados.

Para aplacar tais limita√ß√µes, foram utilizados o m√©todo do cotovelo, o m√©todo K-Means++, a compara√ß√£o do movimento de centroides entre itera√ß√µes e valida√ß√£o por meio do m√©todo da silhueta e do √≠ndice de Davies-Bouldin.

#### M√©todo do cotovelo

A quantidade de clusters a serem utilizados pelo algoritmo deve ser conhecida a priori. O m√©todo do cotovelo[^9] ‚Äî *Elbow method* ‚Äî √© uma forma de obter esse n√∫mero com base na itera√ß√£o entre poss√≠veis centros de clusters e a soma dos quadrados das dist√¢ncias entre eles e os pontos de dados. 

A heur√≠stica opera sob a l√≥gica de que, ao aumentar o n√∫mero de agrupamentos, ocorrer√° a diminui√ß√£o da soma dos quadrados intracluster, haja vista a maior proximidade dos pontos em rela√ß√£o aos centroides de seus respectivos agrupamentos. Em determinado momento, o valor de tal diminui√ß√£o se tornar√° marginal ‚Äî traduzido de maneira visual em gr√°fico, uma linha teria inicialmente quedas acentuadas para, em seguida, se estabilizar na posi√ß√£o horizontal, formando um "cotovelo".

<img src="666.svg" width="100%">

O ponto em que essa estabiliza√ß√£o se torna percept√≠vel representa uma estimativa do n√∫mero ideal de clusters.

#### K-Means++

A determina√ß√£o do n√∫mero de clusters, por√©m, n√£o garante que o algoritmo encontre os melhores pontos para servirem de centroides. A alta sensibilidade da t√©cnica de agrupamento pode levar a uma solu√ß√£o de m√≠nimo local em vez de uma global, gerando parti√ß√µes que n√£o sejam ideais[^10]. 

Para sobrepor tal limita√ß√£o, este trabalho se utilizou do m√©todo de inicializa√ß√£o K-Means++[^11], em que o centroide passa por itera√ß√µes, e √© selecionado a partir da probabilidade de determinado ponto ser o melhor centroide com base na dist√¢ncia em rela√ß√£o aos outros pontos de dados. 

Dado um conjunto de pontos \\(D\\) e um conjunto de centroides j√° selecionados \\(C\\), a probabilidade de se escolher o ponto de dado \\(x\\) como pr√≥ximo centroide √© calculada por meio de

$$
P(x) = \frac{D(x)^2}{\sum_{x^{\prime} \in D}D(x^{\prime})^2}
$$

sendo, \\(D(x)\\): dist√¢ncia entre o ponto \\(x\\) e o centroide mais pr√≥ximo em \\(C\\).

A mudan√ßa sucessiva entre centroides reduz as chances de o algoritmo K-Means convergir para uma solu√ß√£o abaixo do ideal. 

Com os centroides inicializados, cada ponto √© atribu√≠do ao centroide mais pr√≥ximo. Tais pontos de dados pr√≥ximos ao centroide formam clusters ou agrupamentos. Considerando o ponto \\(x\\) e um conjunto de centroides \\(C\\), o r√≥tulo do cluster \\(l\\) ao qual \\(x\\) pertence √© computado por

$$
l(x) = \arg \min_{c \in C}\Vert x - c \Vert
$$

Em seguida, cada centroide √© recalculado tomando a m√©dia da dist√¢ncia de todos os pontos a eles atribu√≠dos,

$$
c_i = \frac{1}{\vert S_i \vert}\sum_{x \in S_i} x
$$

onde, \\(S_i\\): conjunto de todos os pontos atribu√≠dos ao centroide \\(i\\).

A cada itera√ß√£o de atualiza√ß√£o de centroides √© computada a in√©rcia. Para conjunto univariado, 

$$
\sum_{i=1}^{n}{\Vert {x_i} - {c_{l(x_i)}}\Vert}^2
$$

onde, \\(c_{l(x_i)}\\): centroide do cluster para o qual \\(x_i\\) foi atribu√≠do.


#### Crit√©rios aprimorados para converg√™ncia

Al√©m da inicializa√ß√£o por K-Means++, o algoritmo adota crit√©rios de converg√™ncia avan√ßados ao comparar o movimento dos centroides entre itera√ß√µes. Sendo \\(C_t\\) o conjunto de centroides na itera√ß√£o \\(t\\), o algoritmo converge se

$$
\max_{c \in C_t}\Vert c - c_{t - 1} \Vert < tol
$$

onde, 

- \\(\Vert c - c_{t - 1} \Vert\\) dist√¢ncia euclidiana
- \\(tol\\): toler√¢ncia especificada

#### Valida√ß√£o pelo m√©todo da silhueta

A valida√ß√£o dos resultados obtidos a partir da implementa√ß√£o dessas t√©cnicas foi realizada, primeiro, pelo m√©todo da silhueta[^12] ‚Äî *Silhouette method*. Esta t√©cnica observa a similaridade de um ponto com seu cluster em compara√ß√£o com outros clusters a partir de

$$
s_i = \frac{{b_i} - {a_i}}{\max({a_i},{b_i})}
$$

onde:

- \\(a_i\\): dist√¢ncia m√©dia de \\(i\\) para todos os outros pontos intra-agrupamento
- \\(b_i\\): a menor dist√¢ncia m√©dia de \\(i\\) para todos os pontos em agrupamentos diferentes

O m√©todo da silhueta retorna resultados no intervalo de -1 a 1. Se o valor for:

- pr√≥ximo de -1: o ponto est√° agrupado de maneira errada; 
- pr√≥ximo de 0: o ponto est√° entre dois clusters, de forma que o agrupamento pode ser aprimorado; 
- pr√≥ximo de 1: o ponto est√° bem agrupado. 

#### Valida√ß√£o pelo √≠ndice de Davies-Bouldin

Enquanto o m√©todo da silhueta faz compara√ß√£o entre um ponto √∫nico e os agrupamentos, o √≠ndice de Davies-Bouldin[^13], segunda medida usada na valida√ß√£o dos resultados, observa a coes√£o do cluster, dada a l√≥gica de que um agrupamento adequado √© denso em si, ao passo que distante dos demais agrupamentos.

Melhor o agrupamento quanto mais o √≠ndice se aproxima de 0, resultado obtido por

$$
\frac{1}{k}\sum_{i=1}^{k}\max_{i \ne j}\bigg(\frac{{S_i}+{S_j}}{M_{ij}}\bigg)
$$

sendo, 

- \\(k\\): n√∫mero de clusters; 
- \\(i\\),\\(j\\): clusters diferentes; 
- \\(S_i\\), \\(S_j\\): dispers√£o interna dos clusters \\(i\\) e \\(j\\), respectivamente; 
- \\(M_{ij}\\): dist√¢ncia entre clusters \\(i\\) e \\(j\\)

## Resultados

O algoritmo desenvolvido processou as informa√ß√µes de 4.453 registros de despesas em 86 n√∫meros de CNPJ √∫nicos, conforme recorte supracitado conforme os par√¢metros a seguir:

| Par√¢metro | Valor |
|---|---|
| N√∫mero m√≠nimo de clusters | 2 |
| N√∫mero de clusters utilizados | 2 a 10, selecionado pelo m√©todo do cotovelo |
| M√°ximo de itera√ß√µes | 100 |
| Toler√¢ncia para converg√™ncia | 0.0001 |
| Percentil para detec√ß√£o de anomalia |	95 |

Ele retornou 262 anomalias que somam R$ 197.697,24 ‚Äî 11,08% do valor total de despesas. 

Por anomalias entendem-se padr√µes em dados que n√£o se ajustam √† no√ß√£o bem definida de comportamento normal[^14] ‚Äî no contexto deste trabalho, anomalias s√£o valores de despesas que n√£o se enquadram nos agrupamentos criados pelo algoritmo. Tal defini√ß√£o √© importante aqui porque o intuito do trabalho √© fornecer um algoritmo para detec√ß√£o de poss√≠veis fraudes no uso de verbas p√∫blicas. Uma amostra de 12 empresas contendo pouco mais de 10% das anomalias ilustra a l√≥gica de que nem toda anomalia deve ser observada como ind√≠cio de fraude.

<img src="Imagem1.svg" width="100%"><br><small>Nota: empresas identificadas de 1 a 12 para melhor visualiza√ß√£o</small>

H√° anomalias que se encontram no meio de todas as despesas de determinada empresa ‚Äî estas n√£o s√£o os maiores valores no conjunto de despesas e, portanto, s√£o falsos positivos. Na ilustra√ß√£o, as empresas 3 e 12, cujas despesas s√£o de grandes valores, t√™m anomalias, mas dilu√≠das no conjunto de outras despesas, n√£o podendo, assim, ser consideradas poss√≠veis ind√≠cios de fraude; j√° as anomalias das empresas 2, 4 e 6, que t√™m poucas despesas e todas de baixos valores, merecem ser mais bem escrutinadas por √≥rg√£os de controle. 

Em K-Means, a determina√ß√£o de uma anomalia √© feita pela dist√¢ncia dos pontos em rela√ß√£o a um centroide, o que forma um cluster. Na amostra de 12 empresas h√° aquela com 2 clusters (empresa 2) at√© empresas com 8 clusters (empresas 3, 9, 10 e 11), o que explica por que pode haver anomalias dilu√≠das no meio de despesas menores e maiores. 

<img src="Imagem2.svg" width="100%">
<small>Nota: empresas identificadas de 1 a 12 para melhor visualiza√ß√£o</small>

J√° no conjunto de 86 empresas, os clusters v√£o de 2 a 10.

Validamos os agrupamentos por meio de dois instrumentos conforme supracitado: 

1. M√©todo da silhueta, cujos resultados aceit√°veis devem estar entre 0,5 e 1 de uma escala que vai de -1 a 1;
2. √çndice de Davies-Bouldin, com resultados ideais entre 0 a 0,5, numa escala que vai de 0 a 1. 

Na amostra em quest√£o, conseguimos obter valores ideais. 

<img src="Imagem3.svg" width="100%">
<small>Nota: empresas identificadas de 1 a 12 para melhor visualiza√ß√£o</small>

Do conjunto de 86 empresas, todas apresentam resultados ideias para o m√©todo da silhueta (de 0,577 a 0,918); 79 apresentaram resultados ideais para o √≠ndice de Davies-Bouldin (0,166 a 0,489), enquanto 7 apresentaram resultados abaixo do ideal (0,508 a 0,573). 

Com a clusteriza√ß√£o das despesas, a detec√ß√£o de anomalias segundo o algoritmo de K-Means e a valida√ß√£o dos m√©todos aplicados, foi realizada uma an√°lise final para considerar anomalias pass√≠veis de inquiri√ß√£o dos √≥rg√£os de controle aquelas cujos valores s√£o maiores que o maior valor de n√£o anomalia do √∫ltimo cluster. Com isso, descartaram-se anomalias posicionadas entre clusters. 

<img src="Imagem4.svg" width="100%">
<small>Nota: empresas identificadas de 1 a 86 para melhor visualiza√ß√£o</small>

Como resultado final, foram detectadas 46 anomalias em 32 empresas, com valor total de R$ 44.348,88.

{{< expandable label="Empresas e anomalias" level="2" >}}
| CNPJ | Valor (R$) | 
|---|---|
| 02.012.862/0001-60 | 9.584,44 |
| 03.071.465/0001-21 | 1.658,78 |
| 03.300.974/0049-23 | 298,95 |
| 08.402.977/0001-47 | 269,26 |
| 09.060.964/0106-77 | 448,74 |
| 09.060.964/0106-77 | 389,17 |
| 09.399.877/0001-71 | 1.788,63 |
| 09.438.123/0001-83 | 570,85 |
| 09.456.178/0001-16 | 285,66 |
| 09.456.550/0001-94 | 487,44 |
| 09.456.550/0001-94 | 453,99 |
| 09.456.704/0001-48 | 405,21 |
| 09.456.704/0001-48 | 438,34 |
| 09.456.714/0001-83 | 567,66 |
| 09.536.662/0001-55 | 407,22 |
| 11.384.785/0001-60 | 840,34 |
| 13.232.868/0001-69 | 1.683,45 |
| 13.232.868/0001-69 | 1.498,23 |
| 42.591.651/0612-82 | 134,45 |
| 42.591.651/0612-82 | 119,93 |
| 43.386.903/0001-65 | 308,69 |
| 43.386.903/0001-65 | 1.036,99 |
| 43.386.903/0001-65 | 1.361,20 |
| 44.993.632/0001-79 | 1.887,10 |
| 44.993.632/0001-79 | 2.621,23 |
| 44.993.632/0001-79 | 2.218,63 |
| 45.007.937/0001-27 | 1.556,45 |
| 47.079.637/0001-89 | 1.805,09 |
| 49.967.557/0001-95 | 1.777,74 |
| 50.244.235/0001-05 | 108,86 |
| 51.483.956/0001-22 | 184,01 |
| 54.867.247/0001-39 | 216,09 |
| 54.867.247/0001-39 | 447,56 |
| 54.867.247/0001-39 | 359,06 |
| 54.951.561/0001-03 | 239,37 |
| 56.007.859/0001-87 | 593,48 |
| 58.699.232/0001-60 | 218,54 |
| 61.084.018/0001-03 | 1.372,78 |
| 61.359.691/0001-09 | 181,82 |
| 61.563.557/0001-25 | 242,33 |
| 61.980.272/0012-42 | 219,43 |
| 65.684.037/0003-93 | 525,07 |
| 65.684.037/0003-93 | 790,71 |
| 65.684.037/0003-93 | 495,19 |
| 65.684.037/0003-93 | 647,51 |
| 66.728.858/0001-85 | 603,21 |
{{< /expandable >}}

## C√≥digos comentados

### Algoritmo

```py
from typing import Tuple
import numpy as np


class KMeans:
    """
    k-means com crit√©rios de converg√™ncia aprimorados.

    Atributos:
        k (int): N√∫mero de clusters.
        max_iters (int): N√∫mero m√°ximo de itera√ß√µes para o k-means.
        tol (float): Toler√¢ncia de converg√™ncia baseada no movimento do
            centroide.
        n_init (int): N√∫mero de vezes que o algoritmo ser√° executado com
            diferentes seeds de centroides.
        threshold (int): Percentil para detec√ß√£o de anomalias.
        centroids (np.ndarray): Centroides para os clusters.
    """

    def __init__(
        self,
        k: int = 2,
        max_iters: int = 100,
        tol: float = 1e-4,
        n_init: int = 30,
        threshold: int = 95,
        centroids: np.ndarray = None,
    ):
        """
        Inicializa√ß√£o com par√¢metros especificados.
        """
        self.k = k
        self.max_iters = max_iters
        self.tol = tol
        self.n_init = n_init
        self.threshold = threshold
        self.centroids = centroids

    @staticmethod
    def _kpp_init(data: np.ndarray, k: int) -> np.ndarray:
        """
        Inicializa os centroides usando o m√©todo k-means++.

        Argumentos:
            data (np.ndarray): Dados de entrada.
            k (int): N√∫mero de centroides desejados.

        Retorna:
            centroids (np.ndarray): Centroides inicializados.
        """
        # seleciona ponto aleat√≥rio como centroide
        centroids = [data[np.random.choice(len(data))]]

        # itera sobre centroides restantes
        for _ in range(1, k):
            # calcula o quadrado da dist√¢ncia entre cada ponto e o
            # centroide mais pr√≥ximo
            squared_dist = np.array(
                [np.min([np.linalg.norm(c - x) ** 2 for c in centroids]) for x in data]
            )
            # calcula a probabilidade de selecionar cada ponto de dado
            # como novo centroide
            probs = squared_dist / squared_dist.sum()
            # escolhe o ponto com maior probabilidade como novo
            # centroide
            centroid = data[np.argmax(probs)]
            # adiciona novo centroide √† lista de centroides
            centroids.append(centroid)
        return np.array(centroids)

    def get_optimal_k(self, data: np.ndarray, k_max: int = 10) -> int:
        """
        Aplica m√©todo Elbow para obter o n√∫mero de clusters ideal.

        Argumentos:
            data (np.ndarray): Dados usados no algoritmo K-Means.
            k_max (int): N√∫mero m√°ximo de clusters. Valor-padr√£o: 10.

        Retorna:
            optimal_k (int): N√∫mero de clusters ideal.
        """
        # lista para armazenar in√©rcia de cada k
        sum_sq = []
        # itera sobre intervalo de 1 a 10
        for k in range(1, k_max + 1):
            # ajusta o n√∫mero de clusters para a itera√ß√£o atual
            self.k = k
            # ajusta os dados ao algoritmo
            self.fit(data)
            # calcula a in√©rcia
            inertia = np.sum(
                [
                    np.linalg.norm(data[i] - self.centroids[self.labels[i]]) ** 2
                    for i in range(len(data))
                ]
            )
            # adiciona a in√©rcia √† lista
            sum_sq.append(inertia)
        # calcula a diferen√ßa dos valores de in√©rcia para encontrar o
        # cotovelo
        diffs = np.diff(sum_sq, 2)
        # escolhe k ideal a partir da menor diferen√ßa
        optimal_k = np.argmin(diffs) + 1
        return optimal_k

    def _single_run(self, data: np.ndarray) -> Tuple[np.ndarray, np.ndarray, float]:
        """
        Realiza execu√ß√£o √∫nica do algoritmo k-means.

        Argumentos:
            data (np.ndarray): Dados de entrada.

        Retorna:
            centroids (np.ndarray): Melhores centroides ap√≥s a execu√ß√£o
                do k-means.
            labels (np.ndarray): Atribui√ß√µes de cluster para cada ponto
                de dado.
            inertia (float): Dist√¢ncia total dos pontos de dados a
                partir de seus centroides atribu√≠dos.
        """
        # inicializa centoides
        centroids = self._kpp_init(data, self.k)

        # itera sobre max_iters:
        for _ in range(self.max_iters):
            # calcula a dist√¢ncia entre cada ponto e cada centroide
            dist = np.linalg.norm(data[:, np.newaxis] - centroids, axis=2)
            # atribui cada ponto ao centroide mais pr√≥ximo
            labels = np.argmin(dist, axis=1)
            # calcula os novos centroides com base na atribui√ß√£o recente
            new_centroids = np.array(
                [data[labels == i].mean(axis=0) for i in range(self.k)]
            )
            # observa se a mudan√ßa no centroide est√° abaixo da
            # toler√¢ncia
            if np.all(np.abs(new_centroids - centroids) < self.tol):
                # interrompe a itera√ß√£o
                break
            # sobrescreve lista de centroides
            centroids = new_centroids
        # calcula a in√©rcia
        inertia = np.sum(
            [
                np.linalg.norm(data[i] - centroids[labels[i]]) ** 2
                for i in range(len(data))
            ]
        )
        return centroids, labels, inertia

    def fit(self, data: np.ndarray) -> None:
        """
        Ajusta o algoritmo k-means aos dados.

        Argumento:
            data (np.ndarray): Dados de entrada.
        """
        # atribui valor infinito √† in√©rcia m√≠nima
        min_inertia = float("inf")
        # atribui None aos melhores centroides
        best_centroids = None
        # atribui None √†s melhores labels
        best_labels = None

        # itera sobre quantidade de execu√ß√µes de K-Means
        for _ in range(self.n_init):
            # obt√©m valores de centroides, labels, in√©cia
            centroids, labels, inertia = self._single_run(data)
            # observa se a execu√ß√£o atual tem menor in√©rcia
            if inertia < min_inertia:
                # atualiza in√©rcia m√≠nima
                min_inertia = inertia
                # atualiza melhores centroides
                best_centroids = centroids
                # atualiza melhores labels
                best_labels = labels

        # ajusta os valores de centroides para os melhores valores
        # encontrados
        self.centroids = best_centroids
        # ajusta os valores de labels para os melhores valores
        # encontrados
        self.labels = best_labels

    def detect(self, data: np.ndarray) -> np.ndarray:
        """
        Detecta anomalias nos dados com base na dist√¢ncia ao centroide
        mais pr√≥ximo.

        Argumentos:
            data (np.ndarray): Dados de entrada.

        Retorna:
            anomalies (np.ndarray): Anomalias detectadas.
        """
        # calcula a dist√¢ncia entre cada ponto e o centroide mais
        # pr√≥ximo
        dist = np.min(
            np.linalg.norm(data[:, np.newaxis] - self.centroids, axis=2), axis=1
        )
        # ajusta o limite com base no percentil inserido
        threshold = np.percentile(dist, self.threshold)
        # considera anomalias os pontos cujas dist√¢ncias s√£o maiores que
        # o limite
        anomalies = data[dist > threshold]
        return anomalies

    def get_labels(self, data: np.ndarray) -> np.ndarray:
        """
        Atribui cada ponto de dado ao centroide mais pr√≥ximo para
        determinar seu cluster.

        Argumento:
            data (np.ndarray): Conjunto de dados.

        Retorna:
            labels (np.ndarray): Array de labels de cluster
                correspondentes a cada ponto de dado.
        """
        # calcula a dist√¢ncia de cada ponto a cada centroide
        dist = np.linalg.norm(data[:, np.newaxis] - self.centroids, axis=2)
        # atribui cada ponto ao centroide mais pr√≥ximo
        labels = np.argmin(dist, axis=1)
        return labels


class Score:
    """
    C√°lculo de scoring para algoritmo de clusteriza√ß√£o.
    """

    @staticmethod
    def silhouette(data: np.ndarray, labels: np.ndarray) -> float:
        """
        Calcula o score do m√©todo da silhueta.

        Argumentos:
            data (np.ndarray): Dados de entrada.
            labels (np.ndarray): Atribui√ß√µes de cluster para cada ponto
                de dado.

        Retorna:
            float: valor do m√©todo da silhueta.
        """
        # obt√©m labels √∫nicas
        unique_labels = np.unique(labels)
        # lista para armazenar valores do m√©todo da silhueta
        silhouette_vals = []
        # itera sobre pontos de dados
        for index, label in enumerate(labels):
            # obt√©m pontos que est√£o no mesmo cluster
            same_cluster = data[labels == label]
            # calcula a dist√¢ncia m√©dia a outros pontos no mesmo cluster
            a = np.mean(np.linalg.norm(same_cluster - data[index], axis=1))
            # extrai pontos de outros clusters
            other_clusters = [
                data[labels == other_label]
                for other_label in unique_labels
                if other_label != label
            ]
            # calcula a dist√¢ncia m√©dia para pontos em outros clusters
            b_vals = [
                np.mean(np.linalg.norm(cluster - data[index], axis=1))
                for cluster in other_clusters
            ]
            # obt√©m os menores valores
            b = min(b_vals)
            # calcula o valor da silhueta
            silhouette_vals.append((b - a) / max(a, b))
        # retorna a silhueta m√©dia para todos os pontos
        return np.mean(silhouette_vals)

    @staticmethod
    def daviesbouldin(data: np.ndarray, labels: np.ndarray) -> float:
        """
        Calcula o √≠ndice de Davies-Bouldin.

        Argumentos:
            data (np.ndarray): Dados de entrada.
            labels (np.ndarray): Atribui√ß√µes de cluster para cada ponto
                de dado.

        Retorna:
            float: valor de Davies-Bouldin calculado.
        """
        # obt√©m labels √∫nicas
        unique_labels = np.unique(labels)
        # calcula o centroide para cada cluster
        centroids = np.array(
            [data[labels == label].mean(axis=0) for label in unique_labels]
        )
        # calcula a dist√¢ncia m√©dia dentro de cada cluster
        avg_dist_within_cluster = np.array(
            [
                np.mean(
                    np.linalg.norm(data[labels == label] - centroids[label], axis=1)
                )
                for label in unique_labels
            ]
        )
        # calcula a dist√¢ncia entre centroides
        centroid_dist = np.linalg.norm(centroids[:, np.newaxis] - centroids, axis=2)
        # ajusta valores diagonais para infinito
        np.fill_diagonal(centroid_dist, float("inf"))
        # calcula a raz√£o entre a soma das dist√¢ncias m√©dias e a
        # dist√¢ncia entre centroides
        cluster_ratios = (
            avg_dist_within_cluster[:, np.newaxis] + avg_dist_within_cluster
        ) / centroid_dist
        # obt√©m a maior raz√£o para cada cluster
        max_cluster_ratios = np.max(cluster_ratios, axis=1)
        # retorna a m√©dia das maiores raz√µes
        return np.mean(max_cluster_ratios)
```

### Excecu√ß√£o

```py
import os
import asyncio
import glob
from typing import List, Dict, Union
from itertools import groupby
import xml.etree.ElementTree as ET
import aiohttp
from aiolimiter import AsyncLimiter
import pandas as pd
import numpy as np
import sys

sys.path.insert(0, "..")
from src.kmeans import KMeans, Score


async def download_xml(year: int, semaphore: asyncio.Semaphore) -> None:
    """
    Realiza download ass√≠ncrono de xml para um √∫nico ano.

    Argumentos:
        year (int): Ano do arquivo xml.
        semaphore (asyncio.Semaphore): Controlador de acesso concorrente.
    """
    limiter = AsyncLimiter(1, 0.125)
    USER_AGENT = ""
    headers = {"User-Agent": USER_AGENT}
    DATA_DIR = os.path.join(os.getcwd(), "data")
    if not os.path.exists(DATA_DIR):
        os.mkdir(DATA_DIR)
    url = f"https://www.al.sp.gov.br/repositorioDados/deputados/despesas_gabinetes_{str(year)}.xml"
    async with aiohttp.ClientSession(headers=headers) as session:
        await semaphore.acquire()
        async with limiter:
            async with session.get(url) as resp:
                content = await resp.read()
                semaphore.release()
                file = f"despesas_gabinetes_{str(year)}.xml"
                with open(os.path.join(DATA_DIR, file), "wb") as f:
                    f.write(content)


async def fetch_expenses(year_start: int, year_end: int) -> None:
    """
    Realiza download ass√≠ncrono de xml para um per√≠odo.

    Argumentos:
        year_start (int): In√≠cio do per√≠odo.
        year_end (int): Fim do per√≠odo.
    """
    tasks = set()
    semaphore = asyncio.Semaphore(value=10)
    for i in range(int(year_start), int(year_end) + 1):
        task = asyncio.create_task(download_xml(i, semaphore))
        tasks.add(task)
    await asyncio.wait(tasks, return_when=asyncio.ALL_COMPLETED)


def parse_data(list_files: List[str]) -> List[Dict[str, Union[str, None]]]:
    """
    Interpreta dados dos arquivos xml e extrai informa√ß√µes relevantes.

    Argumentos:
        list_files (list): Lista dos caminhos para os arquivos xml.

    Retorna:
        data (list): Lista de dicion√°rios de despesas.
    """
    data = list()
    for file in list_files:
        tree = ET.parse(file)
        xroot = tree.getroot()
        for child in xroot.iter("despesa"):
            cols = [elem.tag for elem in child]
            values = [elem.text for elem in child]
            data.append(dict(zip(cols, values)))
    return data


# executa `fetch_expenses` no per√≠odo de 2013 a 2022
asyncio.run(fetch_expenses(2013, 2022))
# observa se h√° o diret√≥rio `data`
if os.path.exists(os.path.join(os.getcwd(), "data")):
    # acessa diret√≥rio
    os.chdir("data")
    # lista arquivos xml
    files = glob.glob("*.xml")
    # interpreta os arquivos
    load = parse_data(files)
    # armazena os dados na vari√°vel `despesas`
    despesas = pd.DataFrame.from_dict(load, dtype={"Matricula": str, "CNPJ": str})
# leitura dos data de IPCA
ipca = pd.read_csv("../data/ipca.csv")
# convers√£o da vari√°vel Data para datetime
ipca["Data"] = pd.to_datetime(ipca["Data"])
# parseamento da data
despesas["Data"] = pd.to_datetime(
    despesas["Ano"].astype(str) + (despesas["Mes"].astype(str)).str.zfill(2) + "01"
)
# filtro da categoria de despesa
despesas = despesas[
    despesas["Tipo"] == "I - HOSPEDAGEM, ALIMENTA√á√ÉO E DESPESAS DE LOCOMO√á√ÉO"
]
# manuten√ß√£o das colunas estritamente necess√°rias
despesas = despesas[["Data", "CNPJ", "Valor"]]
# filtro a partir de 2018
despesas = despesas[despesas["Data"].dt.year > 2017]
# jun√ß√£o das duas bases
data = pd.merge(left=despesas, right=ipca, on="Data", how="inner")
# ajuste para o valor de dezembro de 2022
data["Valor_ref"] = ipca[ipca["Data"] == "2022-12-01"]["Valor"].values[0]
# c√°lculo da defla√ß√£o
data["Valor_corrigido"] = round(
    (data["Valor_ref"] / data["Valor_y"]) * data["Valor_x"], 2
)
# remo√ß√£o de vari√°veis desnecess√°rias
data = data[["CNPJ", "Valor_corrigido"]]
# remo√ß√£o de linhas com CNPJ nulos
data = data[data["CNPJ"].notnull()]
# filtro para CNPJs com apenas >= 20 entradas
data = data.groupby("CNPJ").filter(lambda x: len(x) >= 20)
# cria√ß√£o de listas para comportar os valores do m√©todo de silhueta e
# √≠ndice de Davies-Bouldin
sils, dbs = list(), list()
# inicializa√ß√£o do algoritmo de K-Means
kmeans = KMeans()
# organiza√ß√£o dos dados
selecao_dados = sorted(zip(data["CNPJ"], data["Valor_corrigido"]), key=lambda x: x[0])
# lista vazia para resultados finais
resultados_lista = []

# itera√ß√£o por CNPJ e cole√ß√£o de despesas
for cnpj, grupo in groupby(selecao_dados, key=lambda x: x[0]):
    # lista vazia de centroides
    centroids_list = []
    # convers√£o para array
    values = np.array([item[1] for item in grupo])
    # obten√ß√£o do k ideal
    kmeans.k = kmeans.get_optimal_k(values.reshape(-1, 1))
    # ajuste de dados ao algoritmo
    kmeans.fit(values.reshape(-1, 1))
    # detec√ß√£o de anomalias
    anomalies_kmeans = kmeans.detect(values.reshape(-1, 1))
    # c√°lculo do m√©todo de silhueta
    silhouette_score = Score.silhouette(
        values.reshape(-1, 1), kmeans.get_labels(values.reshape(-1, 1))
    )
    # c√°lculo do √≠ndice de Davies-Bouldin
    db_score = Score.daviesbouldin(
        values.reshape(-1, 1), kmeans.get_labels(values.reshape(-1, 1))
    )
    # obten√ß√£o de labels
    labels = kmeans.get_labels(values.reshape(-1, 1))
    # itera√ß√£o sobre labels e valores
    for value, label in zip(values, labels):
        # adi√ß√£o de label no dicion√°rio
        centroids_list.append({"centroid": kmeans.centroids[label][0]})
    # contador zerado
    centroid_idx = 0
    # itera√ß√£o sobre despesas
    for value in values:
        # atribui√ß√£o de 1 para anomalia, 0 para n√£o anomalia
        is_anomaly = 1 if value in anomalies_kmeans else 0
        # adi√ß√£o de dicion√°rio na lista final
        resultados_lista.append(
            {
                "CNPJ": cnpj,
                "Valor": value,
                "Anomalia": is_anomaly,
                "Centroide": centroids_list[centroid_idx]["centroid"],
                "Clusters": kmeans.k,
                "Silhueta": silhouette_score,
                "Davies_Bouldin": db_score,
            }
        )
        # incremento do contador
        centroid_idx += 1

# convers√£o dos resultados em dataframe
resultados = pd.DataFrame(resultados_lista)
# salvamento como csv
resultados.to_csv("../prd/resultado.csv", index=False, encoding="utf-8")

```
## Refer√™ncias

[^1]: Assembleia Legislativa do Estado de S√£o Paulo [Alesp]. 1997. Resolu√ß√£o n. 783, de 1¬∞ de julho de 1997. Altera a Resolu√ß√£o n¬∞ 776, de 14/10/1996, que implantou a nova estrutura administrativa, cria o N√∫cleo de Qualidade e institui a verba de gabinete. Dispon√≠vel em: https://www.al.sp.gov.br/repositorio/legislacao/resolucao.alesp/1997/original-resolucao.alesp-783-01.07.1997.html. Acesso em: 19 mar√ßo 2023.

[^2]: Secretaria da Fazenda e Planejamento do Governo do Estado de S√£o Paulo. 2023. √çndices. Dispon√≠vel em: https://portal.fazenda.sp.gov.br/Paginas/Indices.aspx. Acesso em: 26 mar√ßo 2023.

[^3]: Secretaria da Fazenda e Planejamento do Governo do Estado de S√£o Paulo. 2023. Execu√ß√£o or√ßament√°ria e financeira. Dispon√≠vel em: https://www.fazenda.sp.gov.br/sigeolei131/paginas/flexconsdespesa.aspx. Acesso em: 19 mar√ßo 2023.

[^4]: Minist√©rio P√∫blico de S√£o Paulo. 2022. Sistema Eletr√¥nico de Informa√ß√µes. Dispon√≠vel em: https://www.mpsp.mp.br/sei-sistema-eletronico-de-informacoes Acesso em: 26 mar√ßo 2023.

[^5]: Tribunal de Justi√ßa do Estado de S√£o Paulo. 2023. E-SAJ. Dispon√≠vel em: https://esaj.tjsp.jus.br/esaj/portal.do?servico=190090 Acesso em: 24 setembro 2023.

[^6]: Assembleia Legislativa do Estado de S√£o Paulo. 2023. Portal de Dados Abertos. Dispon√≠vel em: https://www.al.sp.gov.br/dados-abertos/recurso/21 Acesso em: 26 mar√ßo 2023.

[^7]: Instituto Brasileiro de Geografia e Estat√≠stica. IPCA. Dispon√≠vel em: https://www.ibge.gov.br/estatisticas/economicas/precos-e-custos/9256-indice-nacional-de-precos-ao-consumidor-amplo.html?=&t=series-historicas Acesso em: 26 mar√ßo 2023.

[^8]: MacQueen, J. 1967. Some methods for classification and analysis of multivariate observations. In: 5th Berkeley Symposium on Mathematical Statistics and Probability, 1967, Los Angeles, LA, Estados Unidos, Anais‚Ä¶ p. 281-297.

[^9]: Joshi, K.D.; Nalwade, P.S. 2012. Modified K-Means for better initial cluster centres. International Journal of Computer Science and Mobile Computing 7: 219-223.

[^10]: Morissette, L.; Chartier, S. 2013. The K-Means clustering technique: General considerations and implementation in Mathematica. Tutorials in Quantitative Methods for Psychology 9: 15-24.

[^11]: Arthur, D.; Vassilvitskii, S. 2007. K-Means++: The advantages of careful seeding. Proceedings of Annual ACM-SIAM Symposium on Discrete Algorithms: 1027-1035.

[^12]: Rousseeuw, P.J. 1987. Silhouettes: A graphical aid to the interpretation and validation of cluster analysis. Journal of Computational and Applied Mathematics 20: 53-65.

[^13]: Davies, D.L.; Bouldin, D.W. 1979. A cluster separation measure. IEEE Transactions on Pattern Analysis and Machine Intelligence 2: 224‚Äì227.

[^14]: Chandola, V; Banerjee, A.; Kumar, V. 2009. Anomaly detection: a survey. Association for Computing Machinery Computing Surveys 41: 1-58.
